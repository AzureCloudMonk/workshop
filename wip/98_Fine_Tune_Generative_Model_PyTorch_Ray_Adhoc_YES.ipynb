{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5498f889-e148-40fc-92a6-14c5f631d6db",
   "metadata": {},
   "source": [
    "# PyTorch\n",
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s below. <==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e458666-ed73-4e01-9a05-80551a46e2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33cb988-575f-48ba-840f-bbb7db8b178c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s above. <==_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cfbb6c-6b1c-4e07-a29b-116b313dc4e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hugging Face Transformers\n",
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s below. <==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bebe0b-d622-46d3-9e75-6fcea355b3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --disable-pip-version-check -q transformers==4.26.1 datasets==2.9.0 accelerate==0.17.0 bitsandbytes==0.37.0 promptsource==0.2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6873c09-aff7-4b94-af46-034ac8b41350",
   "metadata": {},
   "source": [
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s above. <==_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2475c2fd-8828-4573-b764-97cecaa93ec8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ray\n",
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s below. <==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db10d6de-c616-4ec1-bd00-df322a609c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check -q ray==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11060781-020a-4b39-9058-e87bf80cbf09",
   "metadata": {},
   "source": [
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s above. <==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23727304-a3aa-474a-8976-3de1ef2d24d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import Dataset\n",
    "# import ray\n",
    "\n",
    "# lm_dataset_train = Dataset.from_parquet('../data/train/*.parquet')\n",
    "\n",
    "# train_dataset = ray.data.from_huggingface(\n",
    "#     lm_dataset_train\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628e0a6-68d9-4173-b76c-e734ec4611a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We are only testing Causal Language Modeling here\n",
    "model_checkpoint = \"bigscience/bloomz-560m\"\n",
    "\n",
    "def train_function(train_dataset, eval_dataset=None, **config):\n",
    "    # Check that train_dataset has len\n",
    "#    assert len(train_dataset)\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    model = AutoModelForCausalLM.from_config(model_config)\n",
    "    # evaluation_strategy = (\n",
    "    #     config.pop(\"evaluation_strategy\", \"epoch\") if eval_dataset else \"no\"\n",
    "    # )\n",
    "    training_args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-amazon-reviews\",\n",
    "#        evaluation_strategy=config.pop(\"evaluation_strategy\", \"epoch\") if eval_dataset else \"no\",\n",
    "#        logging_strategy=config.pop(\"logging_strategy\", \"epoch\"),\n",
    "        num_train_epochs=config.pop(\"epochs\", 1),\n",
    "        learning_rate=config.pop(\"learning_rate\", 2e-5),\n",
    "        weight_decay=0.01,\n",
    "#        disable_tqdm=True,\n",
    "        no_cuda=True,\n",
    "#        save_strategy=config.pop(\"save_strategy\", \"epoch\"),\n",
    "        **config,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "#        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0797f222-f8b6-48c9-bc1f-823726e3ac68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pytest\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "import ray\n",
    "ray.shutdown()\n",
    "\n",
    "import ray.data\n",
    "from ray.exceptions import RayTaskError\n",
    "from ray.train.huggingface import (\n",
    "    HuggingFaceTrainer,\n",
    "    HuggingFaceCheckpoint,\n",
    ")\n",
    "from ray.air.config import ScalingConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24505489-81be-4c33-a91d-1b3661bf1916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "def ray_start():\n",
    "    address_info = ray.init(num_workers=num_cpus-20)\n",
    "    yield address_info\n",
    "    # The code after the yield will run as teardown code.\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "583ac3ce-9566-4add-8409-26fd9a44a944",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 05:28:25,011\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(num_blocks=1, num_rows=16, schema={input_ids: object, attention_mask: object, labels: object})\n",
      "Dataset(num_blocks=1, num_rows=16, schema={input_ids: object, attention_mask: object, labels: object})\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray_start()\n",
    "\n",
    "# # 16 first rows of tokenized wikitext-2-raw-v1 training & validation\n",
    "train_data = \"\"\"\n",
    "{\"input_ids\":{\"0\":[238,437,222,75,89,337,65,425,716,229,593,266,41,41,238,252,241,213,74,127,173,826,437,222,75,89,337,65,428,466,609,202,67,412,204,425,716,229,593,311,329,422,220,216,69,466,159,151,168,99,150,192,112,148,161,106,148,163,112,148,162,94,148,163,104,148,162,193,148,163,98,148,163,103,148,162,95,19,201,250,211,209,437,222,75,89,337,65,218,198,265,668,234,70,888,428,310,201,597,200,264,237,652,401,227,281,437,222,75,89,337,65,425,716,229,593,266,41,41,573,83,446,329,422,220,201,301,196],\"1\":[194,538,452,494,234,277,535,223,348,446,79,674,934,204,307,742,71,65,225,259,204,427,14,54,719,274,198,290,76,288,51,84,291,290,349,614,209,789,234,620,221,329,220,85,430,546,17,221,329,422,220,201,322,301,198,244,825,674,221,198,437,222,75,89,337,65,843,209,338,77,520,622,223,198,203,386,214,283,233,218,194,538,452,225,237,222,277,582,674,520,288,281,457,224,401,262,382,621,201,198,285,521,358,302,83,224,217,222,234,76,227,198,455,674,225,774,83,198,239,318,251,248,382,239,201,196,224,213],\"2\":[222,230,243,211,430,434,211,553,86,223,198,254,291,218,315,366,427,661,198,241,262,497,338,85,635,220,312,217,516,889,644,202,84,841,394,205,473,524,225,390,224,211,744,872,198,266,77,473,453,434,211,239,247,222,251,208,89,297,336,213,239,209,252,261,674,959,934,408,221,546,16,201,212,217,488,223,547,196,768,411,224,349,233,218,198,630,231,588,263,437,222,75,89,337,65,425,716,229,593,266,41,209,980,515,322,751,890,198,285,325,364,641,208,895,218,198,843,201,322,471,699,87,242,230,512,414,234,486],\"3\":[74,431,967,201,781,281,230,368,223,198,674,604,274,71,272,223,274,843,664,67,249,320,209,425,217,257,280,541,474,199,297,65,211,65,282,200,74,235,225,441,316,199,282,211,316,72,73,241,368,273,279,79,857,751,564,204,340,577,86,632,821,337,216,201,308,362,296,437,222,75,89,337,65,425,716,229,593,266,41,970,210,236,368,216,72,73,339,90,655,65,209,240,768,411,927,218,730,320,232,325,914,198,518,337,381,209,261,674,268,83,919,213,223,642,69,258,203,302,71,307,259,288,268,78,209,252,530,922],\"4\":[296,729,211,388,203,222,216,221,329,422,220,201,225,258,224,314,838,307,857,329,422,220,216,69,225,206,216,612,987,790,209,929,734,626,201,322,891,777,231,469,76,79,256,614,449,242,201,308,362,296,326,875,325,204,159,204,522,221,318,402,576,218,295,544,209,530,258,471,486,978,204,587,537,71,65,225,326,946,222,348,446,79,326,273,291,843,209,300,467,227,250,284,203,222,216,218,437,222,75,89,337,65,425,716,229,593,266,41,201,437,222,75,89,337,65,425,716,229,593,266,41,41,258,410,844,222,968,201],\"5\":[445,196,214,220,877,76,291,441,208,551,234,296,198,674,268,83,875,325,204,159,204,522,258,734,620,221,546,20,209,259,204,427,14,54,719,602,751,564,227,198,214,476,309,854,296,198,934,408,218,437,222,75,89,337,65,466,240,90,447,789,86,270,286,233,274,198,290,76,288,51,84,291,493,209,252,238,238,315,386,520,288,238,238,252,662,296,577,86,632,437,222,75,89,275,65,425,716,229,593,267,728,201,437,222,75,89,337,65,425,716,229,593,266,41,41,301,196,194,538,452,494,234,277,535,223,674,756,535],\"6\":[320,194,657,449,228,76,218,196,230,243,211,430,434,211,225,194,657,540,221,230,638,443,872,554,303,89,274,613,209,403,210,346,390,227,352,718,341,229,219,680,277,250,783,224,220,979,296,326,273,380,878,280,224,349,314,868,201,296,878,690,405,69,368,223,540,73,417,718,348,79,229,204,405,69,262,72,219,463,66,593,225,540,73,417,718,434,86,79,229,204,601,88,84,209,261,535,199,344,71,566,216,718,196,843,218,250,197,383,230,638,443,201,267,82,256,957,434,76,722,204,281,230,422,83,295,684,293,214],\"7\":[202,539,518,786,204,718,225,237,520,288,204,281,543,390,434,76,722,204,209,261,494,933,227,253,532,285,521,844,291,263,198,230,422,348,217,346,897,483,223,263,326,785,272,271,779,535,199,268,83,528,228,532,466,646,496,205,381,233,301,359,791,204,201,198,548,301,359,222,204,555,227,198,535,199,209,339,286,83,446,230,638,443,201,198,535,199,878,690,237,246,221,196,212,600,201,756,434,868,684,293,212,431,249,968,225,878,280,267,859,299,205,438,260,83,209,517,362,83,446,198,906,285,521,230,638,443,390,878],\"8\":[280,277,753,778,747,230,638,443,691,761,227,962,652,242,203,387,256,230,974,320,209,929,198,674,268,83,981,84,233,201,882,522,222,866,335,216,390,434,76,722,204,201,766,218,642,232,336,223,196,232,278,294,962,70,229,512,89,702,244,607,824,221,198,237,246,218,198,674,209,261,202,390,471,250,402,69,203,273,306,291,253,234,967,691,380,227,198,674,268,83,506,906,331,228,828,201,308,910,543,194,657,196,723,89,931,210,494,234,209,252,261,674,268,83,219,668,234,809,246,303,201,198,265,76,73,52,58,809],\"9\":[246,303,201,301,212,217,337,204,547,970,264,340,437,222,75,89,275,65,425,716,229,593,209,300,519,230,638,443,201,535,320,359,791,253,532,434,211,487,223,196,227,80,277,231,469,224,320,80,395,388,218,198,219,668,234,70,888,230,422,466,263,298,196,878,280,301,359,791,204,201,198,535,199,758,216,198,878,280,196,590,198,219,668,234,70,888,221,244,825,277,224,320,200,209,240,878,280,684,669,733,263,298,503,277,194,564,201,445,878,690,684,293,267,82,373,204,230,512,414,234,194,564,83,319,198,875,213,399],\"10\":[218,548,878,690,268,194,564,83,209,338,532,878,280,499,196,214,888,225,231,333,567,218,758,915,250,273,616,307,444,240,557,315,971,411,209,397,80,227,254,415,878,690,684,293,741,474,204,227,196,887,234,230,638,233,209,300,519,674,520,288,201,878,690,206,351,798,573,159,347,203,249,269,72,223,232,65,354,579,227,642,201,781,281,444,331,222,299,634,594,83,311,282,48,310,267,269,84,223,250,284,371,812,656,722,204,573,307,554,303,89,569,394,83,209,338,532,878,280,499,753,778,239,290,279,242,453,83,239],\"11\":[201,203,75,351,83,434,73,387,69,227,253,532,878,280,209,261,89,390,231,272,271,204,587,239,290,320,200,222,290,279,242,453,239,201,400,390,221,78,365,203,75,351,83,295,648,334,434,222,280,204,434,76,382,548,87,854,231,640,380,307,198,285,521,225,684,253,211,294,331,76,80,371,556,80,204,69,196,878,280,201,225,239,265,668,234,290,279,242,453,83,239,201,400,390,267,228,574,718,462,198,674,225,308,683,83,267,82,373,219,79,675,227,196,878,280,209,236,79,409,217,78,265,668,234,290,279,242,453],\"12\":[83,201,253,532,878,280,499,196,434,73,387,69,239,259,226,690,236,614,239,201,196,267,82,271,277,219,620,203,75,351,194,614,295,684,293,788,227,772,387,482,225,250,197,75,962,652,242,203,75,351,83,209,425,217,257,690,471,490,241,80,262,453,240,66,243,839,295,267,82,373,642,194,303,80,210,430,219,79,419,83,263,198,219,668,234,70,888,466,384,260,84,684,733,272,365,239,300,860,703,77,325,239,225,758,69,196,590,198,219,668,234,70,888,296,462,897,234,84,223,342,240,557,290,79,594,267,971,411],\"13\":[201,198,878,280,789,243,65,684,392,347,84,587,468,239,437,222,75,89,337,65,304,477,239,225,527,479,896,197,67,551,234,201,732,266,77,67,65,684,194,217,71,269,230,512,414,234,554,303,89,434,868,296,468,331,336,89,615,422,200,209,252,236,228,343,83,390,231,272,271,204,587,214,388,472,481,216,466,852,462,83,201,808,722,84,228,343,320,201,811,415,320,201,317,220,67,320,225,704,77,356,68,241,636,853,209,236,228,343,320,684,203,87,211,309,472,481,216,307,370,568,223,444,741,474,204,615,422,200],\"14\":[209,425,568,223,472,481,804,216,410,267,682,264,196,465,395,198,285,208,83,267,890,732,221,196,577,86,632,472,481,209,312,287,348,640,521,221,219,668,234,201,377,473,73,507,634,594,83,390,196,603,204,227,198,203,387,256,201,400,390,231,333,608,286,204,587,214,388,962,652,242,569,608,286,216,392,944,307,198,821,482,203,387,256,201,196,641,208,447,962,652,223,340,989,267,728,268,922,72,335,218,231,333,608,286,223,227,962,652,242,434,211,194,998,216,209,252,238,238,290,76,279,238,238,252,261,674,194,368],\"15\":[216,385,571,661,198,241,262,497,338,85,635,220,312,217,209,315,366,406,704,77,89,241,387,256,493,18,18,201,471,876,281,239,261,318,251,248,382,239,201,390,196,224,213,222,230,243,211,430,434,211,441,316,204,218,212,82,273,197,637,201,214,356,474,541,199,690,201,225,230,243,211,430,555,483,320,305,607,237,222,254,728,390,159,199,620,340,198,714,83,225,817,200,555,229,73,417,237,652,401,227,307,725,66,320,209,339,82,68,665,307,198,315,366,406,230,243,211,430,227,889,198,659,231,568,199,435,230,638]},\"attention_mask\":{\"0\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"1\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"2\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"3\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"4\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"5\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"6\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"7\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"8\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"9\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"10\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"11\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"12\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"13\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"14\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"15\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},\"labels\":{\"0\":[238,437,222,75,89,337,65,425,716,229,593,266,41,41,238,252,241,213,74,127,173,826,437,222,75,89,337,65,428,466,609,202,67,412,204,425,716,229,593,311,329,422,220,216,69,466,159,151,168,99,150,192,112,148,161,106,148,163,112,148,162,94,148,163,104,148,162,193,148,163,98,148,163,103,148,162,95,19,201,250,211,209,437,222,75,89,337,65,218,198,265,668,234,70,888,428,310,201,597,200,264,237,652,401,227,281,437,222,75,89,337,65,425,716,229,593,266,41,41,573,83,446,329,422,220,201,301,196],\"1\":[194,538,452,494,234,277,535,223,348,446,79,674,934,204,307,742,71,65,225,259,204,427,14,54,719,274,198,290,76,288,51,84,291,290,349,614,209,789,234,620,221,329,220,85,430,546,17,221,329,422,220,201,322,301,198,244,825,674,221,198,437,222,75,89,337,65,843,209,338,77,520,622,223,198,203,386,214,283,233,218,194,538,452,225,237,222,277,582,674,520,288,281,457,224,401,262,382,621,201,198,285,521,358,302,83,224,217,222,234,76,227,198,455,674,225,774,83,198,239,318,251,248,382,239,201,196,224,213],\"2\":[222,230,243,211,430,434,211,553,86,223,198,254,291,218,315,366,427,661,198,241,262,497,338,85,635,220,312,217,516,889,644,202,84,841,394,205,473,524,225,390,224,211,744,872,198,266,77,473,453,434,211,239,247,222,251,208,89,297,336,213,239,209,252,261,674,959,934,408,221,546,16,201,212,217,488,223,547,196,768,411,224,349,233,218,198,630,231,588,263,437,222,75,89,337,65,425,716,229,593,266,41,209,980,515,322,751,890,198,285,325,364,641,208,895,218,198,843,201,322,471,699,87,242,230,512,414,234,486],\"3\":[74,431,967,201,781,281,230,368,223,198,674,604,274,71,272,223,274,843,664,67,249,320,209,425,217,257,280,541,474,199,297,65,211,65,282,200,74,235,225,441,316,199,282,211,316,72,73,241,368,273,279,79,857,751,564,204,340,577,86,632,821,337,216,201,308,362,296,437,222,75,89,337,65,425,716,229,593,266,41,970,210,236,368,216,72,73,339,90,655,65,209,240,768,411,927,218,730,320,232,325,914,198,518,337,381,209,261,674,268,83,919,213,223,642,69,258,203,302,71,307,259,288,268,78,209,252,530,922],\"4\":[296,729,211,388,203,222,216,221,329,422,220,201,225,258,224,314,838,307,857,329,422,220,216,69,225,206,216,612,987,790,209,929,734,626,201,322,891,777,231,469,76,79,256,614,449,242,201,308,362,296,326,875,325,204,159,204,522,221,318,402,576,218,295,544,209,530,258,471,486,978,204,587,537,71,65,225,326,946,222,348,446,79,326,273,291,843,209,300,467,227,250,284,203,222,216,218,437,222,75,89,337,65,425,716,229,593,266,41,201,437,222,75,89,337,65,425,716,229,593,266,41,41,258,410,844,222,968,201],\"5\":[445,196,214,220,877,76,291,441,208,551,234,296,198,674,268,83,875,325,204,159,204,522,258,734,620,221,546,20,209,259,204,427,14,54,719,602,751,564,227,198,214,476,309,854,296,198,934,408,218,437,222,75,89,337,65,466,240,90,447,789,86,270,286,233,274,198,290,76,288,51,84,291,493,209,252,238,238,315,386,520,288,238,238,252,662,296,577,86,632,437,222,75,89,275,65,425,716,229,593,267,728,201,437,222,75,89,337,65,425,716,229,593,266,41,41,301,196,194,538,452,494,234,277,535,223,674,756,535],\"6\":[320,194,657,449,228,76,218,196,230,243,211,430,434,211,225,194,657,540,221,230,638,443,872,554,303,89,274,613,209,403,210,346,390,227,352,718,341,229,219,680,277,250,783,224,220,979,296,326,273,380,878,280,224,349,314,868,201,296,878,690,405,69,368,223,540,73,417,718,348,79,229,204,405,69,262,72,219,463,66,593,225,540,73,417,718,434,86,79,229,204,601,88,84,209,261,535,199,344,71,566,216,718,196,843,218,250,197,383,230,638,443,201,267,82,256,957,434,76,722,204,281,230,422,83,295,684,293,214],\"7\":[202,539,518,786,204,718,225,237,520,288,204,281,543,390,434,76,722,204,209,261,494,933,227,253,532,285,521,844,291,263,198,230,422,348,217,346,897,483,223,263,326,785,272,271,779,535,199,268,83,528,228,532,466,646,496,205,381,233,301,359,791,204,201,198,548,301,359,222,204,555,227,198,535,199,209,339,286,83,446,230,638,443,201,198,535,199,878,690,237,246,221,196,212,600,201,756,434,868,684,293,212,431,249,968,225,878,280,267,859,299,205,438,260,83,209,517,362,83,446,198,906,285,521,230,638,443,390,878],\"8\":[280,277,753,778,747,230,638,443,691,761,227,962,652,242,203,387,256,230,974,320,209,929,198,674,268,83,981,84,233,201,882,522,222,866,335,216,390,434,76,722,204,201,766,218,642,232,336,223,196,232,278,294,962,70,229,512,89,702,244,607,824,221,198,237,246,218,198,674,209,261,202,390,471,250,402,69,203,273,306,291,253,234,967,691,380,227,198,674,268,83,506,906,331,228,828,201,308,910,543,194,657,196,723,89,931,210,494,234,209,252,261,674,268,83,219,668,234,809,246,303,201,198,265,76,73,52,58,809],\"9\":[246,303,201,301,212,217,337,204,547,970,264,340,437,222,75,89,275,65,425,716,229,593,209,300,519,230,638,443,201,535,320,359,791,253,532,434,211,487,223,196,227,80,277,231,469,224,320,80,395,388,218,198,219,668,234,70,888,230,422,466,263,298,196,878,280,301,359,791,204,201,198,535,199,758,216,198,878,280,196,590,198,219,668,234,70,888,221,244,825,277,224,320,200,209,240,878,280,684,669,733,263,298,503,277,194,564,201,445,878,690,684,293,267,82,373,204,230,512,414,234,194,564,83,319,198,875,213,399],\"10\":[218,548,878,690,268,194,564,83,209,338,532,878,280,499,196,214,888,225,231,333,567,218,758,915,250,273,616,307,444,240,557,315,971,411,209,397,80,227,254,415,878,690,684,293,741,474,204,227,196,887,234,230,638,233,209,300,519,674,520,288,201,878,690,206,351,798,573,159,347,203,249,269,72,223,232,65,354,579,227,642,201,781,281,444,331,222,299,634,594,83,311,282,48,310,267,269,84,223,250,284,371,812,656,722,204,573,307,554,303,89,569,394,83,209,338,532,878,280,499,753,778,239,290,279,242,453,83,239],\"11\":[201,203,75,351,83,434,73,387,69,227,253,532,878,280,209,261,89,390,231,272,271,204,587,239,290,320,200,222,290,279,242,453,239,201,400,390,221,78,365,203,75,351,83,295,648,334,434,222,280,204,434,76,382,548,87,854,231,640,380,307,198,285,521,225,684,253,211,294,331,76,80,371,556,80,204,69,196,878,280,201,225,239,265,668,234,290,279,242,453,83,239,201,400,390,267,228,574,718,462,198,674,225,308,683,83,267,82,373,219,79,675,227,196,878,280,209,236,79,409,217,78,265,668,234,290,279,242,453],\"12\":[83,201,253,532,878,280,499,196,434,73,387,69,239,259,226,690,236,614,239,201,196,267,82,271,277,219,620,203,75,351,194,614,295,684,293,788,227,772,387,482,225,250,197,75,962,652,242,203,75,351,83,209,425,217,257,690,471,490,241,80,262,453,240,66,243,839,295,267,82,373,642,194,303,80,210,430,219,79,419,83,263,198,219,668,234,70,888,466,384,260,84,684,733,272,365,239,300,860,703,77,325,239,225,758,69,196,590,198,219,668,234,70,888,296,462,897,234,84,223,342,240,557,290,79,594,267,971,411],\"13\":[201,198,878,280,789,243,65,684,392,347,84,587,468,239,437,222,75,89,337,65,304,477,239,225,527,479,896,197,67,551,234,201,732,266,77,67,65,684,194,217,71,269,230,512,414,234,554,303,89,434,868,296,468,331,336,89,615,422,200,209,252,236,228,343,83,390,231,272,271,204,587,214,388,472,481,216,466,852,462,83,201,808,722,84,228,343,320,201,811,415,320,201,317,220,67,320,225,704,77,356,68,241,636,853,209,236,228,343,320,684,203,87,211,309,472,481,216,307,370,568,223,444,741,474,204,615,422,200],\"14\":[209,425,568,223,472,481,804,216,410,267,682,264,196,465,395,198,285,208,83,267,890,732,221,196,577,86,632,472,481,209,312,287,348,640,521,221,219,668,234,201,377,473,73,507,634,594,83,390,196,603,204,227,198,203,387,256,201,400,390,231,333,608,286,204,587,214,388,962,652,242,569,608,286,216,392,944,307,198,821,482,203,387,256,201,196,641,208,447,962,652,223,340,989,267,728,268,922,72,335,218,231,333,608,286,223,227,962,652,242,434,211,194,998,216,209,252,238,238,290,76,279,238,238,252,261,674,194,368],\"15\":[216,385,571,661,198,241,262,497,338,85,635,220,312,217,209,315,366,406,704,77,89,241,387,256,493,18,18,201,471,876,281,239,261,318,251,248,382,239,201,390,196,224,213,222,230,243,211,430,434,211,441,316,204,218,212,82,273,197,637,201,214,356,474,541,199,690,201,225,230,243,211,430,555,483,320,305,607,237,222,254,728,390,159,199,620,340,198,714,83,225,817,200,555,229,73,417,237,652,401,227,307,725,66,320,209,339,82,68,665,307,198,315,366,406,230,243,211,430,227,889,198,659,231,568,199,435,230,638]}}\n",
    "\"\"\"\n",
    "\n",
    "validation_data = \"\"\"\n",
    "{\"input_ids\":{\"0\":[238,282,249,217,283,267,251,77,217,283,238,252,282,249,217,283,267,251,77,217,283,201,876,281,198,338,85,635,69,220,250,706,246,199,371,597,200,250,706,246,199,201,301,196,753,346,218,472,655,204,250,706,246,199,340,198,253,226,612,775,76,373,229,339,298,220,201,259,204,211,199,476,69,220,742,65,225,540,83,218,198,265,76,394,742,65,209,530,301,472,316,539,691,380,227,198,764,220,250,706,246,199,201,282,14,196,760,220,283,209,530,977,267,859,227,196,250,213,71,299,218,605,16,212,77,311,289,20],\"1\":[221,310,225,196,230,481,218,605,393,243,432,880,83,311,245,19,250,66,310,201,225,293,819,196,660,80,229,85,435,224,899,218,472,655,83,209,357,250,822,201,198,250,706,246,320,390,841,467,201,669,527,249,223,239,250,706,246,199,237,68,239,263,212,680,223,209,259,761,205,438,260,83,221,198,203,327,495,201,731,67,223,253,71,71,83,400,390,212,217,337,204,307,198,214,303,222,216,274,578,227,196,544,806,232,208,309,223,587,385,858,851,229,768,86,65,69,209,282,249,217,283,267,251,77,217,283,301,196],\"2\":[823,264,928,69,303,204,214,623,201,225,301,206,271,539,212,971,378,84,487,223,250,706,246,199,224,279,83,201,659,264,196,590,198,863,424,984,593,209,252,238,238,300,216,67,337,381,233,238,238,252,282,249,217,283,267,251,77,217,283,301,196,768,411,212,82,431,571,220,201,296,196,219,335,89,250,213,71,299,578,227,605,16,624,273,269,345,311,289,20,221,310,225,615,363,223,578,227,502,459,605,393,243,432,880,83,311,920,459,245,19,250,66,310,201,308,910,198,250,706,246,320,212,971,378,84,221,250,706],\"3\":[246,199,224,279,83,390,487,957,289,19,459,428,24,212,77,311,670,459,916,221,310,908,225,615,363,796,511,629,459,289,511,289,393,71,311,245,511,502,459,493,511,670,250,66,310,209,317,783,548,212,82,431,571,738,201,250,706,246,320,490,196,232,364,377,316,559,234,851,400,543,230,431,203,458,221,371,975,227,267,859,201,221,196,344,67,382,994,253,67,68,89,83,215,311,230,235,76,84,223,310,209,795,977,205,438,260,951,194,273,216,196,544,274,407,426,71,250,706,246,320,201,445,671,202,226,216,227,263],\"4\":[298,253,292,89,245,459,289,818,274,768,71,199,326,273,637,209,252,261,455,224,899,218,224,350,73,343,335,83,301,454,77,204,296,196,768,411,201,281,89,77,77,269,82,452,224,899,218,472,655,83,209,261,768,71,199,496,301,198,239,212,82,283,294,239,201,225,499,159,590,204,254,335,306,216,788,274,212,82,283,72,223,577,89,440,198,548,301,198,239,212,286,280,239,201,400,499,392,217,80,221,78,199,159,204,71,216,201,225,301,788,274,232,636,223,371,194,383,223,198,577,89,209,397,83,957,201,198,409],\"5\":[678,472,655,301,198,212,82,283,294,201,225,198,358,420,301,198,212,286,280,209,252,261,377,316,559,234,851,301,267,757,417,841,467,485,402,69,201,296,405,279,83,295,767,222,216,298,201,225,407,416,284,993,284,209,261,237,68,713,418,741,375,73,380,296,250,706,246,320,669,528,383,83,550,212,680,223,209,795,205,438,260,83,527,848,201,221,250,822,201,198,237,68,224,278,408,196,246,65,88,220,299,197,301,219,549,227,196,344,563,197,981,88,201,445,198,981,88,301,219,228,75,213,578,307,198,331,208,218],\"6\":[212,680,223,201,734,226,223,198,237,68,224,278,408,209,252,261,472,316,328,691,941,218,282,14,267,251,77,217,283,301,198,764,220,250,706,246,199,201,282,249,217,283,196,760,220,283,209,261,506,753,346,390,723,89,203,273,243,217,201,225,684,293,212,973,204,982,778,73,417,201,308,910,232,89,66,82,271,83,390,434,76,965,539,227,205,438,260,221,198,206,627,203,856,444,358,568,216,804,410,547,76,422,209,261,506,753,346,684,293,231,333,223,85,736,307,196,845,218,878,280,333,790,466,252,261,494,246,82],\"7\":[327,218,282,14,196,760,220,283,293,819,496,371,604,405,828,263,198,423,207,320,446,201,400,390,250,394,223,221,282,14,267,251,77,217,283,209,252,261,405,828,263,198,472,655,83,218,282,14,196,760,220,283,390,237,68,371,237,68,277,194,73,354,204,201,732,244,607,218,282,14,267,251,77,217,283,390,305,560,371,305,560,277,194,73,354,204,209,252,261,423,207,320,446,218,198,472,655,218,282,14,196,760,220,283,301,371,220,411,371,237,68,201,732,295,218,282,14,267,251,77,217,283,301,942,251,89,305,560],\"8\":[371,723,89,224,842,237,68,209,252,238,238,317,822,212,89,67,234,238,238,252,304,303,842,282,14,267,251,77,217,283,237,532,359,88,779,230,208,260,379,646,543,490,267,228,574,227,196,212,217,422,571,250,213,71,299,218,672,16,459,672,21,955,273,269,345,311,428,511,245,459,428,511,428,221,310,201,756,226,230,222,216,230,208,447,319,196,203,76,420,264,947,366,199,203,529,69,209,259,761,194,998,229,417,205,438,260,83,221,203,327,495,708,196,489,242,264,230,235,76,744,214,303,842,201,305,607,598,800],\"9\":[301,817,717,749,678,201,225,196,232,364,277,598,800,204,230,842,209,261,214,303,842,212,217,337,216,198,253,71,71,83,274,578,227,837,904,299,83,201,897,483,223,263,198,194,303,473,208,447,201,569,257,458,227,468,224,234,343,335,83,209,304,303,222,216,212,217,488,223,253,71,71,83,390,203,810,227,293,239,219,199,337,204,239,225,684,293,824,718,462,198,544,209,252,261,253,71,71,83,232,208,309,319,254,420,201,225,198,768,86,65,69,203,87,273,227,198,206,475,833,70,571,756,543,231,337,678,296,198],\"10\":[205,298,220,212,260,592,745,201,577,89,223,263,159,90,79,343,76,858,851,209,795,285,491,896,270,86,216,697,230,235,76,745,225,250,404,83,274,916,459,428,21,792,83,209,929,198,244,825,230,235,76,84,201,198,536,85,86,213,515,194,368,216,263,196,740,472,316,199,227,198,486,512,201,225,486,343,745,196,219,242,72,229,250,347,328,89,234,209,261,536,85,86,213,243,216,390,765,202,264,359,213,221,198,206,627,201,225,390,634,210,264,876,201,308,910,543,390,876,227,293,212,422,614,218,231,278,71,223],\"11\":[972,579,388,219,260,859,83,209,530,301,928,273,380,295,669,245,768,86,65,221,253,292,89,464,562,802,833,86,272,216,227,198,219,242,72,229,224,72,626,209,312,787,543,237,532,196,212,217,422,571,250,213,71,299,218,916,230,77,311,796,511,502,25,221,310,201,198,536,85,86,213,243,216,409,429,444,219,260,859,83,225,285,367,444,486,512,250,272,216,209,252,238,238,300,333,608,286,233,238,238,252,282,249,217,283,267,251,77,217,283,301,824,772,973,198,254,663,277,253,226,612,775,76,373,229,339,298,220],\"12\":[340,254,349,294,78,318,210,683,227,198,240,90,79,345,225,259,210,375,67,79,201,410,867,198,265,222,84,229,742,65,209,530,301,471,846,242,221,659,218,198,259,204,211,199,476,69,220,742,65,201,669,230,638,223,340,198,203,395,233,253,404,218,247,202,563,201,225,308,362,669,198,254,663,277,206,328,767,404,218,198,265,76,394,742,65,209,261,254,349,294,78,77,419,883,306,524,390,824,221,198,318,210,591,71,406,214,74,412,83,236,89,83,70,74,412,213,225,318,412,70,636,65,201,221,83,446,198,704],\"13\":[321,229,247,275,67,234,209,252,261,753,346,684,293,231,272,271,204,587,784,267,213,269,229,417,231,333,197,321,883,306,524,201,496,206,271,216,80,202,256,883,306,291,201,225,697,400,490,231,829,71,204,231,467,227,947,366,253,465,395,388,883,306,291,203,529,216,201,729,83,551,264,231,467,227,486,978,291,227,198,844,222,554,86,275,200,408,209,261,455,218,198,399,301,198,883,306,291,218,250,706,246,320,340,254,349,294,78,318,210,683,201,400,490,504,237,652,401,227,281,198,239,230,271,78,420,277,203,302],\"14\":[250,706,246,199,239,209,261,883,306,524,221,198,259,204,211,199,476,69,220,742,65,390,231,333,197,321,340,244,607,221,198,775,76,373,229,339,298,220,209,261,250,404,231,333,197,321,883,306,291,301,824,221,198,318,269,294,508,83,466,203,251,520,216,340,198,339,419,320,67,195,352,69,360,231,333,197,321,340,244,607,713,791,204,221,198,318,663,742,65,371,811,76,424,425,786,248,209,252,775,84,303,381,83,490,504,746,227,450,228,501,298,282,14,267,251,77,217,283,227,752,903,69,222,325,201,308,362,83],\"15\":[446,548,338,85,635,69,220,753,346,781,281,198,159,204,551,234,212,314,66,201,247,220,67,199,224,330,260,283,209,265,269,687,323,16,20,225,323,17,20,201,496,955,233,250,706,246,199,768,86,65,69,360,734,620,340,232,208,67,294,346,221,300,302,204,197,201,445,198,753,346,992,410,527,479,928,372,76,736,817,209,252,238,238,338,67,727,89,238,238,252,240,68,512,282,14,267,251,77,217,283,250,388,263,198,963,242,222,598,76,70,319,897,299,83,218,796,459,916,16,922,345,311,796,459,493,25,18,214]},\"attention_mask\":{\"0\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"1\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"2\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"3\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"4\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"5\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"6\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"7\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"8\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"9\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"10\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"11\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"12\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"13\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"14\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1],\"15\":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},\"labels\":{\"0\":[238,282,249,217,283,267,251,77,217,283,238,252,282,249,217,283,267,251,77,217,283,201,876,281,198,338,85,635,69,220,250,706,246,199,371,597,200,250,706,246,199,201,301,196,753,346,218,472,655,204,250,706,246,199,340,198,253,226,612,775,76,373,229,339,298,220,201,259,204,211,199,476,69,220,742,65,225,540,83,218,198,265,76,394,742,65,209,530,301,472,316,539,691,380,227,198,764,220,250,706,246,199,201,282,14,196,760,220,283,209,530,977,267,859,227,196,250,213,71,299,218,605,16,212,77,311,289,20],\"1\":[221,310,225,196,230,481,218,605,393,243,432,880,83,311,245,19,250,66,310,201,225,293,819,196,660,80,229,85,435,224,899,218,472,655,83,209,357,250,822,201,198,250,706,246,320,390,841,467,201,669,527,249,223,239,250,706,246,199,237,68,239,263,212,680,223,209,259,761,205,438,260,83,221,198,203,327,495,201,731,67,223,253,71,71,83,400,390,212,217,337,204,307,198,214,303,222,216,274,578,227,196,544,806,232,208,309,223,587,385,858,851,229,768,86,65,69,209,282,249,217,283,267,251,77,217,283,301,196],\"2\":[823,264,928,69,303,204,214,623,201,225,301,206,271,539,212,971,378,84,487,223,250,706,246,199,224,279,83,201,659,264,196,590,198,863,424,984,593,209,252,238,238,300,216,67,337,381,233,238,238,252,282,249,217,283,267,251,77,217,283,301,196,768,411,212,82,431,571,220,201,296,196,219,335,89,250,213,71,299,578,227,605,16,624,273,269,345,311,289,20,221,310,225,615,363,223,578,227,502,459,605,393,243,432,880,83,311,920,459,245,19,250,66,310,201,308,910,198,250,706,246,320,212,971,378,84,221,250,706],\"3\":[246,199,224,279,83,390,487,957,289,19,459,428,24,212,77,311,670,459,916,221,310,908,225,615,363,796,511,629,459,289,511,289,393,71,311,245,511,502,459,493,511,670,250,66,310,209,317,783,548,212,82,431,571,738,201,250,706,246,320,490,196,232,364,377,316,559,234,851,400,543,230,431,203,458,221,371,975,227,267,859,201,221,196,344,67,382,994,253,67,68,89,83,215,311,230,235,76,84,223,310,209,795,977,205,438,260,951,194,273,216,196,544,274,407,426,71,250,706,246,320,201,445,671,202,226,216,227,263],\"4\":[298,253,292,89,245,459,289,818,274,768,71,199,326,273,637,209,252,261,455,224,899,218,224,350,73,343,335,83,301,454,77,204,296,196,768,411,201,281,89,77,77,269,82,452,224,899,218,472,655,83,209,261,768,71,199,496,301,198,239,212,82,283,294,239,201,225,499,159,590,204,254,335,306,216,788,274,212,82,283,72,223,577,89,440,198,548,301,198,239,212,286,280,239,201,400,499,392,217,80,221,78,199,159,204,71,216,201,225,301,788,274,232,636,223,371,194,383,223,198,577,89,209,397,83,957,201,198,409],\"5\":[678,472,655,301,198,212,82,283,294,201,225,198,358,420,301,198,212,286,280,209,252,261,377,316,559,234,851,301,267,757,417,841,467,485,402,69,201,296,405,279,83,295,767,222,216,298,201,225,407,416,284,993,284,209,261,237,68,713,418,741,375,73,380,296,250,706,246,320,669,528,383,83,550,212,680,223,209,795,205,438,260,83,527,848,201,221,250,822,201,198,237,68,224,278,408,196,246,65,88,220,299,197,301,219,549,227,196,344,563,197,981,88,201,445,198,981,88,301,219,228,75,213,578,307,198,331,208,218],\"6\":[212,680,223,201,734,226,223,198,237,68,224,278,408,209,252,261,472,316,328,691,941,218,282,14,267,251,77,217,283,301,198,764,220,250,706,246,199,201,282,249,217,283,196,760,220,283,209,261,506,753,346,390,723,89,203,273,243,217,201,225,684,293,212,973,204,982,778,73,417,201,308,910,232,89,66,82,271,83,390,434,76,965,539,227,205,438,260,221,198,206,627,203,856,444,358,568,216,804,410,547,76,422,209,261,506,753,346,684,293,231,333,223,85,736,307,196,845,218,878,280,333,790,466,252,261,494,246,82],\"7\":[327,218,282,14,196,760,220,283,293,819,496,371,604,405,828,263,198,423,207,320,446,201,400,390,250,394,223,221,282,14,267,251,77,217,283,209,252,261,405,828,263,198,472,655,83,218,282,14,196,760,220,283,390,237,68,371,237,68,277,194,73,354,204,201,732,244,607,218,282,14,267,251,77,217,283,390,305,560,371,305,560,277,194,73,354,204,209,252,261,423,207,320,446,218,198,472,655,218,282,14,196,760,220,283,301,371,220,411,371,237,68,201,732,295,218,282,14,267,251,77,217,283,301,942,251,89,305,560],\"8\":[371,723,89,224,842,237,68,209,252,238,238,317,822,212,89,67,234,238,238,252,304,303,842,282,14,267,251,77,217,283,237,532,359,88,779,230,208,260,379,646,543,490,267,228,574,227,196,212,217,422,571,250,213,71,299,218,672,16,459,672,21,955,273,269,345,311,428,511,245,459,428,511,428,221,310,201,756,226,230,222,216,230,208,447,319,196,203,76,420,264,947,366,199,203,529,69,209,259,761,194,998,229,417,205,438,260,83,221,203,327,495,708,196,489,242,264,230,235,76,744,214,303,842,201,305,607,598,800],\"9\":[301,817,717,749,678,201,225,196,232,364,277,598,800,204,230,842,209,261,214,303,842,212,217,337,216,198,253,71,71,83,274,578,227,837,904,299,83,201,897,483,223,263,198,194,303,473,208,447,201,569,257,458,227,468,224,234,343,335,83,209,304,303,222,216,212,217,488,223,253,71,71,83,390,203,810,227,293,239,219,199,337,204,239,225,684,293,824,718,462,198,544,209,252,261,253,71,71,83,232,208,309,319,254,420,201,225,198,768,86,65,69,203,87,273,227,198,206,475,833,70,571,756,543,231,337,678,296,198],\"10\":[205,298,220,212,260,592,745,201,577,89,223,263,159,90,79,343,76,858,851,209,795,285,491,896,270,86,216,697,230,235,76,745,225,250,404,83,274,916,459,428,21,792,83,209,929,198,244,825,230,235,76,84,201,198,536,85,86,213,515,194,368,216,263,196,740,472,316,199,227,198,486,512,201,225,486,343,745,196,219,242,72,229,250,347,328,89,234,209,261,536,85,86,213,243,216,390,765,202,264,359,213,221,198,206,627,201,225,390,634,210,264,876,201,308,910,543,390,876,227,293,212,422,614,218,231,278,71,223],\"11\":[972,579,388,219,260,859,83,209,530,301,928,273,380,295,669,245,768,86,65,221,253,292,89,464,562,802,833,86,272,216,227,198,219,242,72,229,224,72,626,209,312,787,543,237,532,196,212,217,422,571,250,213,71,299,218,916,230,77,311,796,511,502,25,221,310,201,198,536,85,86,213,243,216,409,429,444,219,260,859,83,225,285,367,444,486,512,250,272,216,209,252,238,238,300,333,608,286,233,238,238,252,282,249,217,283,267,251,77,217,283,301,824,772,973,198,254,663,277,253,226,612,775,76,373,229,339,298,220],\"12\":[340,254,349,294,78,318,210,683,227,198,240,90,79,345,225,259,210,375,67,79,201,410,867,198,265,222,84,229,742,65,209,530,301,471,846,242,221,659,218,198,259,204,211,199,476,69,220,742,65,201,669,230,638,223,340,198,203,395,233,253,404,218,247,202,563,201,225,308,362,669,198,254,663,277,206,328,767,404,218,198,265,76,394,742,65,209,261,254,349,294,78,77,419,883,306,524,390,824,221,198,318,210,591,71,406,214,74,412,83,236,89,83,70,74,412,213,225,318,412,70,636,65,201,221,83,446,198,704],\"13\":[321,229,247,275,67,234,209,252,261,753,346,684,293,231,272,271,204,587,784,267,213,269,229,417,231,333,197,321,883,306,524,201,496,206,271,216,80,202,256,883,306,291,201,225,697,400,490,231,829,71,204,231,467,227,947,366,253,465,395,388,883,306,291,203,529,216,201,729,83,551,264,231,467,227,486,978,291,227,198,844,222,554,86,275,200,408,209,261,455,218,198,399,301,198,883,306,291,218,250,706,246,320,340,254,349,294,78,318,210,683,201,400,490,504,237,652,401,227,281,198,239,230,271,78,420,277,203,302],\"14\":[250,706,246,199,239,209,261,883,306,524,221,198,259,204,211,199,476,69,220,742,65,390,231,333,197,321,340,244,607,221,198,775,76,373,229,339,298,220,209,261,250,404,231,333,197,321,883,306,291,301,824,221,198,318,269,294,508,83,466,203,251,520,216,340,198,339,419,320,67,195,352,69,360,231,333,197,321,340,244,607,713,791,204,221,198,318,663,742,65,371,811,76,424,425,786,248,209,252,775,84,303,381,83,490,504,746,227,450,228,501,298,282,14,267,251,77,217,283,227,752,903,69,222,325,201,308,362,83],\"15\":[446,548,338,85,635,69,220,753,346,781,281,198,159,204,551,234,212,314,66,201,247,220,67,199,224,330,260,283,209,265,269,687,323,16,20,225,323,17,20,201,496,955,233,250,706,246,199,768,86,65,69,360,734,620,340,232,208,67,294,346,221,300,302,204,197,201,445,198,753,346,992,410,527,479,928,372,76,736,817,209,252,238,238,338,67,727,89,238,238,252,240,68,512,282,14,267,251,77,217,283,250,388,263,198,963,242,222,598,76,70,319,897,299,83,218,796,459,916,16,922,345,311,796,459,493,25,18,214]}}\n",
    "\"\"\"\n",
    "\n",
    "train_df = pd.read_json(train_data)\n",
    "validation_df = pd.read_json(validation_data)\n",
    "\n",
    "ray_train = ray.data.from_pandas(train_df)\n",
    "ray_validation = ray.data.from_pandas(validation_df)\n",
    "print(ray_train)\n",
    "print(ray_validation)\n",
    "\n",
    "\n",
    "# from datasets import Dataset\n",
    "# import ray\n",
    "\n",
    "# lm_dataset_train = Dataset.from_parquet('../data/train/*.parquet', cache_dir='/tmp/')\n",
    "\n",
    "# ray_train_ds = ray.data.from_huggingface(\n",
    "#     lm_dataset_train\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ab943f61-aac1-4874-9aed-66c3f64a9b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`dataset` must be a `datasets.Dataset` or `datasets.DatasetDict`, got <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3cf8e476ec1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m ray_train_ds = ray.data.from_huggingface(\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;34m'../data/train/*.parquet'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#lm_dataset_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ray/data/read_api.py\u001b[0m in \u001b[0;36mfrom_huggingface\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1446\u001b[0m         raise TypeError(\n\u001b[0;32m-> 1447\u001b[0;31m             \u001b[0;34m\"`dataset` must be a `datasets.Dataset` or `datasets.DatasetDict`, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m             \u001b[0;34mf\"got {type(dataset)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: `dataset` must be a `datasets.Dataset` or `datasets.DatasetDict`, got <class 'str'>"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58f8f50a-e9cb-43d1-8175-2a6aa6de41e8",
   "metadata": {},
   "source": [
    "# Train model and save a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0499e26-22e9-4000-88f1-bc4230bb17a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'bigscience/bloomz-560m'\n",
    "\n",
    "def train_function(train_dataset, eval_dataset=None, **config):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "    evaluation_strategy = (\n",
    "        config.pop(\"evaluation_strategy\", \"epoch\") if eval_dataset else \"no\"\n",
    "    )\n",
    "    training_args = TrainingArguments(\n",
    "        f\"{model_checkpoint}-amazon-customer-reviews-bloomz-560m\",\n",
    "#        evaluation_strategy=evaluation_strategy,\n",
    "#        logging_strategy=config.pop(\"logging_strategy\", \"epoch\"),\n",
    "        max_steps=10,\n",
    "        num_train_epochs=config.pop(\"epochs\", 1),\n",
    "        learning_rate=config.pop(\"learning_rate\", 2e-5),\n",
    "#        weight_decay=0.01,\n",
    "        disable_tqdm=True,\n",
    "        no_cuda=True,\n",
    "        save_strategy=config.pop(\"save_strategy\", \"no\"),\n",
    "        **config,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d881d6f-de14-4c76-8637-8c5265b7e6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ray/train/base_trainer.py:361: UserWarning: Executing `.fit()` may leave less than 20% of CPUs in this cluster for Dataset execution, which can lead to resource contention or hangs. To avoid this, reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer scaling_config. See https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune for more info.\n",
      "  trainable=trainable, param_space=param_space, run_config=self.run_config\n",
      "2023-03-15 05:28:58,238\tWARNING callback.py:109 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-03-15 05:29:36</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:38.25        </td></tr>\n",
       "<tr><td>Memory:      </td><td>176.8/373.7 GiB    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/96 CPUs, 0/0 GPUs, 0.0/315.75 GiB heap, 0.0/45.26 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                             </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_49150_00000</td><td style=\"text-align: right;\">           1</td><td>/root/ray_results/HuggingFaceTrainer_2023-03-15_05-28-58/HuggingFaceTrainer_49150_00000_0_2023-03-15_05-28-58/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_49150_00000</td><td>ERROR   </td><td>169.255.255.2:49655</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m 2023-03-15 05:29:08,985\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=76]\n",
      "\u001b[2m\u001b[36m(HuggingFaceTrainer pid=49655)\u001b[0m 2023-03-15 05:29:10,989\tINFO bulk_executor.py:39 -- Executing DAG InputDataBuffer[Input] -> AllToAllOperator[randomize_block_order]\n",
      "\u001b[2m\u001b[36m(HuggingFaceTrainer pid=49655)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(HuggingFaceTrainer pid=49655)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49930)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49930)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49934)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49934)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49928)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49928)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49927)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49927)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49964)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49964)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49931)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49931)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49977)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49977)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49986)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49986)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49937)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49937)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49954)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49954)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49974)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49974)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49932)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49932)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49941)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49941)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49951)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49951)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49948)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49948)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49925)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49925)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50006)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50006)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49962)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49962)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49990)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49990)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49968)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49968)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50013)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50013)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49957)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49957)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49945)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49945)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49996)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49996)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49933)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49933)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50001)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50001)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49926)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49926)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49981)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49981)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49983)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49983)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52237)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52237)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49929)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49929)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50018)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50018)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50289)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50289)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50003)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50003)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49998)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49998)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49970)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49970)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50016)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50016)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49994)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49994)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49979)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49979)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49985)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49985)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50095)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50095)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50008)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50008)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50009)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50009)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53572)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53572)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50026)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50026)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50020)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50020)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52340)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52340)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53125)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53125)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53894)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53894)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53846)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53846)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53580)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53580)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53817)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53817)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53583)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53583)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53647)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53647)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53651)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53651)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53726)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53726)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53653)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53653)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53463)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53463)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53657)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53657)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53579)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53579)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53801)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53801)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53889)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53889)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=55031)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=55031)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53897)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53897)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53751)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53751)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=56206)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=56206)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58351)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58351)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=57966)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=57966)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58436)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58436)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58434)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58434)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58433)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58433)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59091)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59091)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59885)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59885)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=63175)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=63175)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=62068)\u001b[0m /opt/conda/lib/python3.7/site-packages/ray/data/_internal/bulk_dataset_iterator.py:109: UserWarning: session.get_dataset_shard returns a ray.data.DatasetIterator instead of a Dataset as of Ray v2.3. Use iter_torch_batches(), to_tf(), or iter_batches() to iterate over one epoch. See https://docs.ray.io/en/latest/data/api/dataset_iterator.html for full DatasetIterator docs.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=62068)\u001b[0m   \"session.get_dataset_shard returns a ray.data.DatasetIterator \"\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49941)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49941)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49977)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49977)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49990)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49990)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49934)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49934)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=55031)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=55031)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50001)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50001)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53579)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53579)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=57966)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=57966)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59091)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59091)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=62068)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=62068)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49957)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49957)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49985)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49985)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53894)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53894)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53889)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53889)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53125)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53125)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49932)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49932)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49981)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49981)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49945)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49945)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53580)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53580)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49951)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49951)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49948)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49948)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53657)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53657)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49927)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49927)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49931)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49931)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53897)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53897)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58434)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58434)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m max_steps is given, it will override any value given in num_train_epochs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50006)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50006)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53583)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53583)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58436)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58436)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58433)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58433)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49974)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49974)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49962)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49962)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50018)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50018)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49998)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49998)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49986)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49986)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49954)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49954)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50009)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50009)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50020)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50020)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53726)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53726)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53751)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53751)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49937)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49937)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49926)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49926)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52237)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52237)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53846)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53846)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50013)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50013)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53572)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53572)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52340)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52340)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49964)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49964)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49929)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49929)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50016)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50016)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50026)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50026)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53817)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53817)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53647)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53647)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53653)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53653)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59885)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59885)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=63175)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=63175)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49933)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49933)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49983)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49983)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53463)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53463)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49925)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49925)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50003)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50003)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=56206)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=56206)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49928)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49928)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49930)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49930)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49970)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49970)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49979)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49979)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53801)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53801)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49996)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49996)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50095)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50095)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49968)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49968)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50289)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50289)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49994)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49994)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50008)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50008)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53651)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53651)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58351)\u001b[0m /opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58351)\u001b[0m   FutureWarning,\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49998)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59091)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50018)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49932)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53897)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49954)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49957)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49933)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50001)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49981)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49979)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50009)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53572)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50020)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53726)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53463)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53657)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=55031)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=59885)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50006)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49925)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49990)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49945)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52237)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49929)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49985)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50095)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53846)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53580)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53801)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=57966)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58433)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49937)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=63175)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49964)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49970)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53647)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49927)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49968)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50008)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53583)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58351)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49928)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m ***** Running training *****\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   Num examples = 608\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   Num Epochs = 9223372036854775807\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   Instantaneous batch size per device = 8\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   Total train batch size (w. parallel, distributed & accumulation) = 608\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   Total optimization steps = 1\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m   Number of trainable parameters = 559214592\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49951)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49996)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50016)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53125)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53894)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49986)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53817)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53653)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49977)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50289)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49941)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58436)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50003)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49931)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49962)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53579)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=56206)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50026)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=62068)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49934)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=50013)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49926)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49983)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53751)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53651)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=58434)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49930)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49974)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m Saving model checkpoint to bigscience/bloomz-560m-wikitext2/checkpoint-0\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49948)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49994)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=52340)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=53889)\u001b[0m There seems to be not a single sample in your epoch_iterator, stopping training at step 0! This is expected if you're using an IterableDataset and set num_steps (1) higher than the number of available samples.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m Configuration saved in bigscience/bloomz-560m-wikitext2/checkpoint-0/config.json\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=49924)\u001b[0m Configuration saved in bigscience/bloomz-560m-wikitext2/checkpoint-0/generation_config.json\n",
      "2023-03-15 05:29:36,415\tERROR trial_runner.py:1062 -- Trial HuggingFaceTrainer_49150_00000: Error processing event.\n",
      "ray.exceptions.RayTaskError(ZeroDivisionError): \u001b[36mray::_Inner.train()\u001b[39m (pid=49655, ip=169.255.255.2, repr=HuggingFaceTrainer)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ray/tune/trainable/trainable.py\", line 368, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(ZeroDivisionError): \u001b[36mray::RayTrainWorker._RayTrainWorker__execute()\u001b[39m (pid=49933, ip=169.255.255.2, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fd8ae02e050>)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/worker_group.py\", line 31, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/ray/train/huggingface/huggingface_trainer.py\", line 417, in _huggingface_train_loop_per_worker\n",
      "    trainer.train()\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1547, in train\n",
      "    ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1915, in _inner_training_loop\n",
      "    train_loss = self._total_loss_scalar / self.state.global_step\n",
      "ZeroDivisionError: float division by zero\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>date               </th><th>experiment_id                   </th><th>hostname                                                   </th><th>node_ip      </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">   trial_id</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_49150_00000</td><td>2023-03-15_05-29-01</td><td>9a43b3801df94a66be2a984daa028726</td><td>datascience-1-0-ml-m5-24xlarge-d807f334101db4b858a3c6bf1fdd</td><td>169.255.255.2</td><td style=\"text-align: right;\">49655</td><td style=\"text-align: right;\"> 1678858141</td><td style=\"text-align: right;\">49150_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 05:29:36,529\tERROR tune.py:794 -- Trials did not complete: [HuggingFaceTrainer_49150_00000]\n",
      "2023-03-15 05:29:36,530\tINFO tune.py:799 -- Total run time: 38.30 seconds (38.22 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "RayTaskError(ZeroDivisionError)",
     "evalue": "\u001b[36mray::_Inner.train()\u001b[39m (pid=49655, ip=169.255.255.2, repr=HuggingFaceTrainer)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/tune/trainable/trainable.py\", line 368, in train\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n    ray.get(object_ref)\nray.exceptions.RayTaskError(ZeroDivisionError): \u001b[36mray::RayTrainWorker._RayTrainWorker__execute()\u001b[39m (pid=49933, ip=169.255.255.2, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fd8ae02e050>)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/worker_group.py\", line 31, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n    train_func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/huggingface/huggingface_trainer.py\", line 417, in _huggingface_train_loop_per_worker\n    trainer.train()\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1547, in train\n    ignore_keys_for_eval=ignore_keys_for_eval,\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1915, in _inner_training_loop\n    train_loss = self._total_loss_scalar / self.state.global_step\nZeroDivisionError: float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(ZeroDivisionError)\u001b[0m           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f64e46ef084b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/ray/train/base_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTuneError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTrainingFailedError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError(ZeroDivisionError)\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=49655, ip=169.255.255.2, repr=HuggingFaceTrainer)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/tune/trainable/trainable.py\", line 368, in train\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n    ray.get(object_ref)\nray.exceptions.RayTaskError(ZeroDivisionError): \u001b[36mray::RayTrainWorker._RayTrainWorker__execute()\u001b[39m (pid=49933, ip=169.255.255.2, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fd8ae02e050>)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/worker_group.py\", line 31, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n    train_func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/ray/train/huggingface/huggingface_trainer.py\", line 417, in _huggingface_train_loop_per_worker\n    trainer.train()\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1547, in train\n    ignore_keys_for_eval=ignore_keys_for_eval,\n  File \"/opt/conda/lib/python3.7/site-packages/transformers/trainer.py\", line 1915, in _inner_training_loop\n    train_loss = self._total_loss_scalar / self.state.global_step\nZeroDivisionError: float division by zero"
     ]
    }
   ],
   "source": [
    "scaling_config = ScalingConfig(num_workers=num_cpus-20, \n",
    "                               use_gpu=False,\n",
    "#                               _max_cpu_fraction_per_node = 0.8    \n",
    ")\n",
    "\n",
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=train_function,\n",
    "    trainer_init_config={\"epochs\": 1, \"save_strategy\": \"no\"},\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ray_train,\n",
    "              #\"evaluation\": ray_validation\n",
    "             },\n",
    ")\n",
    "\n",
    "result = trainer.fit()\n",
    "\n",
    "print(result.checkpoint)\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e8e0e6-107e-4b98-bde5-aad156cf5e9d",
   "metadata": {},
   "source": [
    "# Predict from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82b926-a82e-4f0b-8c72-6417dd40aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = pd.DataFrame(\n",
    "    [\"Complete me\", \"And me\", \"Please complete\"], columns=[\"sentences\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d25fc8f-459c-4e2f-a192-8e44c209cf40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from ray.train.huggingface import (\n",
    "    HuggingFacePredictor,\n",
    ")\n",
    "\n",
    "model_checkpoint = \"bigscience/bloomz-560m\"\n",
    "\n",
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    result.checkpoint,\n",
    "    HuggingFacePredictor,\n",
    "    task=\"text-generation\",\n",
    "    tokenizer=AutoTokenizer.from_pretrained(model_checkpoint)\n",
    ")\n",
    "\n",
    "predictions = predictor.predict(ray.data.from_pandas(prompts))\n",
    "print(predictions)\n",
    "assert predictions.count() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d637dadc-db79-4d8b-aaf9-3e2a0306753c",
   "metadata": {},
   "source": [
    "# Train with an evaluation metric such as \"glue\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b44c72-df60-4f7e-8c50-98b34d2aa493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"glue\", \"cola\")\n",
    "\n",
    "def train_function_with_metric(train_dataset, eval_dataset=None, **config):\n",
    "    print(metric)\n",
    "    return train_function(train_dataset, eval_dataset=eval_dataset, **config)\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=num_cpus-2, use_gpu=False)\n",
    "\n",
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=train_function_with_metric,\n",
    "    trainer_init_config={\"epochs\": 1},\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ray_train, \"evaluation\": ray_validation},\n",
    ")\n",
    "trainer.fit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f1b1d-e0a0-415c-877a-c7e331dfb799",
   "metadata": {},
   "source": [
    "# Resume training from a checkpoint using more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7157133-e712-45c3-bc61-9402247bd7bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer2 = HuggingFaceTrainer(\n",
    "#     trainer_init_per_worker=train_function,\n",
    "#     trainer_init_config={\n",
    "#         \"epochs\": 1,\n",
    "#         \"save_strategy\": \"epoch\",\n",
    "#     },  # this will train for 1 epoch: 5 - 4 = 1\n",
    "#     scaling_config=scaling_config,\n",
    "#     datasets={\"train\": ray_train, \"evaluation\": ray_validation},\n",
    "#     resume_from_checkpoint=result.checkpoint,\n",
    "# )\n",
    "# result2 = trainer2.fit()\n",
    "\n",
    "# assert result2.metrics[\"epoch\"] == 1\n",
    "# assert result2.metrics[\"training_iteration\"] == 1\n",
    "# assert result2.checkpoint\n",
    "# assert isinstance(result2.checkpoint, HuggingFaceCheckpoint)\n",
    "# assert \"eval_loss\" in result2.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956e1096-954a-44b6-b7de-542c8fd60e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor = BatchPredictor.from_checkpoint(\n",
    "#     result2.checkpoint,\n",
    "#     HuggingFacePredictor,\n",
    "#     task=\"text-generation\",\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(model_checkpoint),\n",
    "# )\n",
    "\n",
    "# predictions = predictor.predict(ray.data.from_pandas(prompts))\n",
    "# assert predictions.count() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542c15ab-6d6a-4aa7-9a96-28c3644c87be",
   "metadata": {},
   "source": [
    "# Test hyper-parameter tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d36c82-bf03-4db8-a4e7-6018962dd4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tests if checkpointing and restoring during tuning works correctly.\n",
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler\n",
    "from ray.tune.schedulers.resource_changing_scheduler import (\n",
    "    DistributeResources,\n",
    "    ResourceChangingScheduler,\n",
    ")\n",
    "\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_cpus-2, \n",
    "    use_gpu=False, \n",
    "    trainer_resources={\"CPU\": 0},\n",
    "#    _max_cpu_fraction_per_node = 0.8    \n",
    ")\n",
    "\n",
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=train_function,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": ray_train, \"evaluation\": ray_validation},\n",
    ")\n",
    "\n",
    "tune_epochs = 1\n",
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"trainer_init_config\": {\n",
    "            \"learning_rate\": tune.loguniform(2e-6, 2e-5),\n",
    "            \"epochs\": tune_epochs,\n",
    "            \"save_strategy\": \"epoch\",\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"eval_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=3,\n",
    "        scheduler=ResourceChangingScheduler(\n",
    "            ASHAScheduler(\n",
    "                max_t=tune_epochs,\n",
    "            ),\n",
    "            resources_allocation_function=DistributeResources(\n",
    "                add_bundles=True, reserve_resources={\"CPU\": 1}\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "tuner_results = tuner.fit()\n",
    "print(tuner_results)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.m5.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
