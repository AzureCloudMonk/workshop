{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train reward model with human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=802916929536, available=796064526336, percent=0.9, used=2468024320, free=761958158336, active=2660704256, inactive=36107153408, buffers=0, cached=38490746880, shared=1355776, slab=1071583232)\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "notebook_memory = psutil.virtual_memory()\n",
    "print(notebook_memory)\n",
    "\n",
    "if notebook_memory.total < 32 * 1000 * 1000 * 1000:\n",
    "    print('*******************************************')    \n",
    "    print('YOU ARE NOT USING THE CORRECT INSTANCE TYPE')\n",
    "    print('PLEASE CHANGE INSTANCE TYPE TO  m5.2xlarge ')\n",
    "    print('*******************************************')\n",
    "else:\n",
    "    correct_instance_type=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --disable-pip-version-check -q \\\n",
    "    transformers==4.26.1 \\\n",
    "    datasets==2.9.0 \\\n",
    "    accelerate==0.17.0 \\\n",
    "    bitsandbytes==0.37.0 \\\n",
    "    promptsource==0.2.3 \\\n",
    "    trl==0.4.1 \\\n",
    "    evaluate==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Amazon Python SDK clients\n",
    "sagemaker = boto3.client(\"sagemaker\", region)\n",
    "a2i = boto3.client(\"sagemaker-a2i-runtime\")\n",
    "s3 = boto3.client(\"s3\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the `human_loops_started`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r human_loops_started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    human_loops_started\n",
    "except NameError:\n",
    "    print(\"*** PLEASE RUN PREVIOUS NOTEBOOK BEFORE CONTINUING ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['94503827-403b-4554-8d95-48099c9a37a9', '43b29d80-3662-42eb-845a-833e644aea70', 'fccd9e3b-3cf3-4544-8823-a5a321bf5148', '0519da28-3dfe-45bc-b88b-2ddf0100846d']\n"
     ]
    }
   ],
   "source": [
    "print(human_loops_started)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the Human Loops are Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HumanLoop Name: 94503827-403b-4554-8d95-48099c9a37a9\n",
      "HumanLoop Status: Completed\n",
      "HumanLoop Output Destination: {'OutputS3Uri': 's3://sagemaker-us-east-1-079002598131/ground-truth-star-rating-results/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2/2023/03/26/21/17/40/94503827-403b-4554-8d95-48099c9a37a9/output.json'}\n",
      "\n",
      "Completed!\n",
      "\n",
      "HumanLoop Name: 43b29d80-3662-42eb-845a-833e644aea70\n",
      "HumanLoop Status: Completed\n",
      "HumanLoop Output Destination: {'OutputS3Uri': 's3://sagemaker-us-east-1-079002598131/ground-truth-star-rating-results/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2/2023/03/26/21/17/40/43b29d80-3662-42eb-845a-833e644aea70/output.json'}\n",
      "\n",
      "Completed!\n",
      "\n",
      "HumanLoop Name: fccd9e3b-3cf3-4544-8823-a5a321bf5148\n",
      "HumanLoop Status: Completed\n",
      "HumanLoop Output Destination: {'OutputS3Uri': 's3://sagemaker-us-east-1-079002598131/ground-truth-star-rating-results/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2/2023/03/26/21/17/40/fccd9e3b-3cf3-4544-8823-a5a321bf5148/output.json'}\n",
      "\n",
      "Completed!\n",
      "\n",
      "HumanLoop Name: 0519da28-3dfe-45bc-b88b-2ddf0100846d\n",
      "HumanLoop Status: Completed\n",
      "HumanLoop Output Destination: {'OutputS3Uri': 's3://sagemaker-us-east-1-079002598131/ground-truth-star-rating-results/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2/2023/03/26/21/17/40/0519da28-3dfe-45bc-b88b-2ddf0100846d/output.json'}\n",
      "\n",
      "Completed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(f\"HumanLoop Name: {human_loop_name}\")\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print(\"\")\n",
    "    while resp[\"HumanLoopStatus\"] != \"Completed\":\n",
    "        print(f\"Waiting for HumanLoop to complete.\")\n",
    "        time.sleep(10)\n",
    "        resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)\n",
    "        print(f\"Completed!\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# View Human Labels  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Once the work is complete, Amazon GroundTruth stores the results in the specified S3 bucket and sends a Cloudwatch Event.  Here is a sample item labeled with GroundTruth in `jsonlines` format:\n",
    "```\n",
    "{\n",
    " \"inputContent\": {\"taskObject\": {\n",
    "                         \"prompt\": \"Sometimes it works but usually not\",\n",
    "                         \"responses\": [2, 3]}\n",
    "                 },\n",
    " \"humanAnswers\": [{\"answerContent\": {\n",
    "                        \"ranking_1\": \"1\", # ranking for 1st response (1 is High)\n",
    "                        \"ranking_2\": \"2\"  # ranking for 2nd response (2 is Low)\n",
    "                 }}]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare human-labeled data for RL/PPO training\n",
    "Retrieve from GrountTruth and convert to a binary reward (-1, 1) for all rankings as follows:\n",
    "\n",
    "From this:\n",
    "```\n",
    "prompt                                response    ranking\n",
    "\n",
    "Sometimes it works but usually not    2           1   # High\n",
    "Sometimes it works but usually not    3           2   # Low\n",
    "```\n",
    "\n",
    "To this:\n",
    "```\n",
    "prompt                                response    ranking\n",
    "\n",
    "Sometimes it works but usually not    2           1   # Ranked highest\n",
    "Sometimes it works but usually not    3           0   # Not ranked highest\n",
    "```\n",
    "\n",
    "To this (`turn_into_text_classification_format()` below):\n",
    "```\n",
    "prompt                                response    highest_ranked_response_index\n",
    "\n",
    "Sometimes it works but usually not    [2,3]       0   # 0th item in the response list is ranked the highest\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# _Note:  If nothing is showing up below, you need to return to finish the previous notebook by labeling the data in Ground Truth!!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'flowDefinitionArn': 'arn:aws:sagemaker:us-east-1:079002598131:flow-definition/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2', 'humanAnswers': [{'acceptanceTime': '2023-03-26T21:18:14.324Z', 'answerContent': {'response_1_ranking': '2', 'response_2_ranking': '1'}, 'submissionTime': '2023-03-26T21:18:39.397Z', 'timeSpentInSeconds': 25.073, 'workerId': 'e7232bf5ab67e176', 'workerMetadata': {'identityData': {'identityProviderType': 'Cognito', 'issuer': 'https://cognito-idp.us-east-1.amazonaws.com/us-east-1_WEvHYVrSh', 'sub': '06e39925-66bd-45b1-b495-9081ad730e85'}}}], 'humanLoopName': '94503827-403b-4554-8d95-48099c9a37a9', 'inputContent': {'taskObject': {'prompt': 'I enjoy this product', 'responses': [4, 5]}}}\n",
      "{'flowDefinitionArn': 'arn:aws:sagemaker:us-east-1:079002598131:flow-definition/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2', 'humanAnswers': [{'acceptanceTime': '2023-03-26T21:18:46.813Z', 'answerContent': {'response_1_ranking': '1', 'response_2_ranking': '2'}, 'submissionTime': '2023-03-26T21:18:55.376Z', 'timeSpentInSeconds': 8.563, 'workerId': 'e7232bf5ab67e176', 'workerMetadata': {'identityData': {'identityProviderType': 'Cognito', 'issuer': 'https://cognito-idp.us-east-1.amazonaws.com/us-east-1_WEvHYVrSh', 'sub': '06e39925-66bd-45b1-b495-9081ad730e85'}}}], 'humanLoopName': '43b29d80-3662-42eb-845a-833e644aea70', 'inputContent': {'taskObject': {'prompt': 'I am unhappy with this product', 'responses': [1, 2]}}}\n",
      "{'flowDefinitionArn': 'arn:aws:sagemaker:us-east-1:079002598131:flow-definition/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2', 'humanAnswers': [{'acceptanceTime': '2023-03-26T21:18:39.468Z', 'answerContent': {'response_1_ranking': '1', 'response_2_ranking': '2'}, 'submissionTime': '2023-03-26T21:18:46.748Z', 'timeSpentInSeconds': 7.28, 'workerId': 'e7232bf5ab67e176', 'workerMetadata': {'identityData': {'identityProviderType': 'Cognito', 'issuer': 'https://cognito-idp.us-east-1.amazonaws.com/us-east-1_WEvHYVrSh', 'sub': '06e39925-66bd-45b1-b495-9081ad730e85'}}}], 'humanLoopName': 'fccd9e3b-3cf3-4544-8823-a5a321bf5148', 'inputContent': {'taskObject': {'prompt': 'It is okay', 'responses': [3, 4]}}}\n",
      "{'flowDefinitionArn': 'arn:aws:sagemaker:us-east-1:079002598131:flow-definition/fd-dsoaws-star-rating-9e22832c-50fa-42c4-aa9f-507a31473ec2', 'humanAnswers': [{'acceptanceTime': '2023-03-26T21:18:55.436Z', 'answerContent': {'response_1_ranking': '1', 'response_2_ranking': '2'}, 'submissionTime': '2023-03-26T21:19:06.567Z', 'timeSpentInSeconds': 11.131, 'workerId': 'e7232bf5ab67e176', 'workerMetadata': {'identityData': {'identityProviderType': 'Cognito', 'issuer': 'https://cognito-idp.us-east-1.amazonaws.com/us-east-1_WEvHYVrSh', 'sub': '06e39925-66bd-45b1-b495-9081ad730e85'}}}], 'humanLoopName': '0519da28-3dfe-45bc-b88b-2ddf0100846d', 'inputContent': {'taskObject': {'prompt': 'Sometimes it works but usually not', 'responses': [2, 3]}}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "human_feedback_items = []\n",
    "\n",
    "for resp in completed_human_loops:\n",
    "    human_feedback_s3_uri = resp[\"HumanLoopOutput\"][\"OutputS3Uri\"]\n",
    "    split_string = re.split(\"s3://\" + bucket + \"/\", resp[\"HumanLoopOutput\"][\"OutputS3Uri\"])\n",
    "    key = split_string[1]\n",
    "    \n",
    "    response = s3.get_object(Bucket=bucket, Key=key)\n",
    "    content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "    json_output = json.loads(content)\n",
    "    print(json_output)\n",
    "\n",
    "    prompt = json_output[\"inputContent\"]['taskObject']['prompt']\n",
    "    responses = json_output[\"inputContent\"]['taskObject']['responses']\n",
    "    response_1_ranking = json_output[\"humanAnswers\"][0][\"answerContent\"]['response_1_ranking']\n",
    "    response_2_ranking = json_output[\"humanAnswers\"][0][\"answerContent\"]['response_2_ranking']\n",
    "    \n",
    "    human_feedback_item_1 = (prompt, responses[0], response_1_ranking)\n",
    "    human_feedback_items.append(human_feedback_item_1)\n",
    "    human_feedback_item_2 = (prompt, responses[1], response_2_ranking)\n",
    "    human_feedback_items.append(human_feedback_item_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I enjoy this product</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I enjoy this product</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am unhappy with this product</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am unhappy with this product</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is okay</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is okay</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sometimes it works but usually not</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sometimes it works but usually not</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               prompt  response ranking\n",
       "0                I enjoy this product         4       2\n",
       "1                I enjoy this product         5       1\n",
       "2      I am unhappy with this product         1       1\n",
       "3      I am unhappy with this product         2       2\n",
       "4                          It is okay         3       1\n",
       "5                          It is okay         4       2\n",
       "6  Sometimes it works but usually not         2       1\n",
       "7  Sometimes it works but usually not         3       2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human_feedback_items = pd.DataFrame(human_feedback_items, columns=['prompt', 'response', 'ranking'])\n",
    "df_human_feedback_items.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Convert ranking into 0 or 1 reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I enjoy this product</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I enjoy this product</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am unhappy with this product</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am unhappy with this product</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is okay</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>It is okay</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sometimes it works but usually not</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sometimes it works but usually not</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               prompt response ranking\n",
       "0                I enjoy this product        4       0\n",
       "1                I enjoy this product        5       1\n",
       "2      I am unhappy with this product        1       1\n",
       "3      I am unhappy with this product        2       0\n",
       "4                          It is okay        3       1\n",
       "5                          It is okay        4       0\n",
       "6  Sometimes it works but usually not        2       1\n",
       "7  Sometimes it works but usually not        3       0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rankings = 2\n",
    "df_human_feedback_items['response'] = df_human_feedback_items['response'].apply(lambda response: str(response))\n",
    "df_human_feedback_items['ranking'] = df_human_feedback_items['ranking'].apply(lambda ranking: str(abs(int(ranking) - num_rankings)))\n",
    "df_human_feedback_items.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am unhappy with this product</td>\n",
       "      <td>1,2</td>\n",
       "      <td>1,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I enjoy this product</td>\n",
       "      <td>4,5</td>\n",
       "      <td>0,1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is okay</td>\n",
       "      <td>3,4</td>\n",
       "      <td>1,0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sometimes it works but usually not</td>\n",
       "      <td>2,3</td>\n",
       "      <td>1,0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               prompt response ranking\n",
       "0      I am unhappy with this product      1,2     1,0\n",
       "1                I enjoy this product      4,5     0,1\n",
       "2                          It is okay      3,4     1,0\n",
       "3  Sometimes it works but usually not      2,3     1,0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human_feedback_items_grouped_by_prompt = df_human_feedback_items.groupby('prompt', as_index=False).agg({'prompt' : 'first', 'response' : ','.join, 'ranking' : ','.join})\n",
    "df_human_feedback_items_grouped_by_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>response</th>\n",
       "      <th>ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I am unhappy with this product</td>\n",
       "      <td>[1, 2]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I enjoy this product</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>[0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is okay</td>\n",
       "      <td>[3, 4]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sometimes it works but usually not</td>\n",
       "      <td>[2, 3]</td>\n",
       "      <td>[1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               prompt response ranking\n",
       "0      I am unhappy with this product   [1, 2]  [1, 0]\n",
       "1                I enjoy this product   [4, 5]  [0, 1]\n",
       "2                          It is okay   [3, 4]  [1, 0]\n",
       "3  Sometimes it works but usually not   [2, 3]  [1, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_human_feedback_items_grouped_by_prompt['response'] = df_human_feedback_items_grouped_by_prompt['response'].apply(lambda response: [int(s) for s in response.split(',')])\n",
    "df_human_feedback_items_grouped_by_prompt['ranking'] = df_human_feedback_items_grouped_by_prompt['ranking'].apply(lambda ranking: [int(s) for s in ranking.split(',')])\n",
    "df_human_feedback_items_grouped_by_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'ranking'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create Dataset objects (Arrow PyTables) from Pandas dataframes\n",
    "human_feedback_dataset = Dataset.from_pandas(df_human_feedback_items_grouped_by_prompt)\n",
    "human_feedback_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a reward model with human preference and alignment data\n",
    "This is typically a language model initialized from the supervised-fine-tuned (SFT) model (trained in a previous notebook), but with an additional binary-classification layer placed on top.  This reward model is used to train the reinforcement-learning model in the next step.  The reinforcement-learning model is what is deployed into production to serve applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:  This should be bloomz (not bloom) at this point in the flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_checkpoint = 'bigscience/bloomz-560m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_checkpoint\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the notebooks in the previous section before you continue.\")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigscience/bloomz-560m\n"
     ]
    }
   ],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r supervised_fine_tuned_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    supervised_fine_tuned_model_path\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the notebooks in the previous section before you continue.\")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tmp_models/bigscience/bloom-560m/\n"
     ]
    }
   ],
   "source": [
    "print(supervised_fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulated (cooking show) output from SFT (Step 1) for Amazon Customer Review generative-classification task\n",
    "#model_name = \"bigscience/bloomz-560m\" \n",
    "\n",
    "# Load the human comparisons dataset for tuning the reward model.\n",
    "# Simulated output from HF Step 2 to be used to train reward for \"helpfulness\"\n",
    "#ds = load_dataset(\"openai/summarize_from_feedback\", name=\"comparisons\") \n",
    "\n",
    "# Load the value-head model and tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn the dataset into pairs of prompt + responses, where text_j is the preferred prompt + response and text_k is the other.\n",
    "def turn_into_text_classification_format(examples):\n",
    "    new_examples = {\"text_j\": [], \"text_k\": []}\n",
    "    for prompt, response, ranking in zip(examples[\"prompt\"], examples[\"response\"], examples[\"ranking\"]):\n",
    "        # TODO:  Add a check to make sure there is only a single 0 and a single 1\n",
    "        if len(response) != 2 or len(ranking) != 2 or ranking[0] not in (0, 1) or ranking[1] not in (0, 1):\n",
    "            raise ValueError(\n",
    "                f\"There should be two responses with a ranking that is either 0 or 1. Received {len(response)} responses and {len(ranking)} rankings.\"\n",
    "            )\n",
    "            \n",
    "        highest_ranked_response_index = ranking.index(1)\n",
    "\n",
    "        new_examples[\"text_j\"].append(\n",
    "            str(response[highest_ranked_response_index]) + \" \" + tokenizer.bos_token + \" \" + prompt\n",
    "        )\n",
    "        new_examples[\"text_k\"].append(\n",
    "            str(response[0 if highest_ranked_response_index == 1 else 1]) + \" \" + tokenizer.bos_token + \" \" + prompt\n",
    "        )\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "# Tokenize the dataset.\n",
    "def preprocess_function(examples):\n",
    "    tokenized_j = tokenizer(examples[\"text_j\"], truncation=True)\n",
    "    tokenized_k = tokenizer(examples[\"text_k\"], truncation=True)\n",
    "    return {\n",
    "        \"input_ids_j\": tokenized_j[\"input_ids\"],\n",
    "        \"attention_mask_j\": tokenized_j[\"attention_mask\"],\n",
    "        \"input_ids_k\": tokenized_k[\"input_ids\"],\n",
    "        \"attention_mask_k\": tokenized_k[\"attention_mask\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prompt', 'response', 'ranking']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "#1:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#2:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#0: 100%|██████████| 1/1 [00:00<00:00, 93.81ba/s]A\n",
      "#1: 100%|██████████| 1/1 [00:00<00:00, 114.21ba/s]\n",
      "#2: 100%|██████████| 1/1 [00:00<00:00, 127.15ba/s]\n",
      "#3: 100%|██████████| 1/1 [00:00<00:00, 159.15ba/s]\n",
      "num_proc must be <= 4. Reducing num_proc to 4 for dataset of size 4.\n",
      "#0:   0%|          | 0/1 [00:00<?, ?ba/s]\n",
      "\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "#2:   0%|          | 0/1 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "#0: 100%|██████████| 1/1 [00:00<00:00, 148.19ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#2: 100%|██████████| 1/1 [00:00<00:00, 122.81ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "#1: 100%|██████████| 1/1 [00:00<00:00, 153.40ba/s]\n",
      "#3: 100%|██████████| 1/1 [00:00<00:00, 215.62ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids_j', 'attention_mask_j', 'input_ids_k', 'attention_mask_k'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "num_proc = 8  # Can adjust to be higher if you have more processors. Should work even if you don't have 8 CPUs, though.\n",
    "original_columns = human_feedback_dataset.column_names\n",
    "print(original_columns)\n",
    "\n",
    "human_feedback_binary_classification_dataset = human_feedback_dataset.map(turn_into_text_classification_format, batched=True, num_proc=num_proc, remove_columns=original_columns)\n",
    "\n",
    "human_feedback_tokenized_dataset = human_feedback_binary_classification_dataset.map(preprocess_function, \n",
    "                                                                                    batched=True, \n",
    "                                                                                    num_proc=num_proc, \n",
    "                                                                                    remove_columns=[\"text_j\", \"text_k\"])\n",
    "\n",
    "print(human_feedback_tokenized_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloomz-560m and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1)\n",
    "\n",
    "# # Need to do this for gpt2, because it doesn't have an official pad token.\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the metric that we'll use for validation.\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, _ = eval_pred\n",
    "    # Here, predictions is rewards_j and rewards_k.\n",
    "    # We want to see how much of the time rewards_j > rewards_k.\n",
    "    predictions = np.argmax(predictions, axis=0)\n",
    "    labels = np.zeros(predictions.shape)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We need to define a special data collator that batches the data in our j vs k format.\n",
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        features_j = []\n",
    "        features_k = []\n",
    "        for feature in features:\n",
    "            features_j.append({\"input_ids\": feature[\"input_ids_j\"], \"attention_mask\": feature[\"attention_mask_j\"]})\n",
    "            features_k.append({\"input_ids\": feature[\"input_ids_k\"], \"attention_mask\": feature[\"attention_mask_k\"]})\n",
    "        batch_j = self.tokenizer.pad(\n",
    "            features_j,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch_k = self.tokenizer.pad(\n",
    "            features_k,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids_j\": batch_j[\"input_ids\"],\n",
    "            \"attention_mask_j\": batch_j[\"attention_mask\"],\n",
    "            \"input_ids_k\": batch_k[\"input_ids\"],\n",
    "            \"attention_mask_k\": batch_k[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 559215616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-27 00:09:33.749: W smdistributed/modelparallel/torch/nn/predefined_hooks.py:78] Found unsupported HuggingFace version 4.26.1 for automated tensor parallelism. HuggingFace modules will not be automatically distributed. You can use smp.tp_register_with_module API to register desired modules for tensor parallelism, or directly instantiate an smp.nn.DistributedModule. Supported HuggingFace transformers versions for automated tensor parallelism: ['4.17.0', '4.20.1', '4.21.0']\n",
      "[2023-03-27 00:09:33.786 pytorch-1-12-gpu--ml-p3dn-24xlarge-307ebad80d11874f5dcc2ce687db:7284 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-03-27 00:09:33.913 pytorch-1-12-gpu--ml-p3dn-24xlarge-307ebad80d11874f5dcc2ce687db:7284 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "/opt/conda/lib/python3.8/site-packages/smdebug-1.0.24b20230214-py3.8.egg/smdebug/profiler/system_metrics_reader.py:78: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to bigscience_bloomz-560m_summarization_reward_model/checkpoint-1\n",
      "Configuration saved in bigscience_bloomz-560m_summarization_reward_model/checkpoint-1/config.json\n",
      "Model weights saved in bigscience_bloomz-560m_summarization_reward_model/checkpoint-1/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./tmp_models/reward_model/\n",
      "Configuration saved in ./tmp_models/reward_model/config.json\n",
      "Model weights saved in ./tmp_models/reward_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "class RewardTrainer(Trainer):\n",
    "    # Define how to compute the reward loss.\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards_j = model(input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n",
    "        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n",
    "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "        return loss\n",
    "\n",
    "# Define and parse arguments.\n",
    "local_rank = 0\n",
    "resume_from_checkpoint = False\n",
    "deepspeed = None\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "gradient_accumulation_steps = 4\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.001\n",
    "bf16 = False\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Define the training args. Needs to be done before the model is loaded if you are using deepspeed.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_checkpoint.replace('/', '_')}_reward_model\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "#    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#    deepspeed=deepspeed,\n",
    "#    local_rank=local_rank,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    ")\n",
    "    \n",
    "# Train the model, woohoo.\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=human_feedback_tokenized_dataset, #[\"train\"],\n",
    "#    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model_path = './tmp_models/reward_model/'\n",
    "\n",
    "trainer.save_model(reward_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'reward_model_path' (str)\n"
     ]
    }
   ],
   "source": [
    "%store reward_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./tmp_models/reward_model/config.json\n",
      "Model config BloomConfig {\n",
      "  \"_name_or_path\": \"./tmp_models/reward_model/\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "loading weights file ./tmp_models/reward_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BloomForSequenceClassification.\n",
      "\n",
      "All the weights of BloomForSequenceClassification were initialized from the model checkpoint at ./tmp_models/reward_model/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def create_list_input_files(path):\n",
    "#     input_files = glob.glob('{}/*.parquet'.format(path))\n",
    "#     print(input_files)\n",
    "#     return input_files\n",
    "\n",
    "# def save_transformer_model(model, model_dir):\n",
    "#     path = os.path.join(model_dir, 'transformer')\n",
    "#     os.makedirs(path, exist_ok=True)                              \n",
    "#     print('Saving Transformer model to {}'.format(path))\n",
    "#     model.save_pretrained(path)\n",
    "\n",
    "# def save_pytorch_model(model, model_checkpoint, model_dir):\n",
    "#     os.makedirs(model_dir, exist_ok=True) \n",
    "#     print('Saving PyTorch model to {}'.format(model_dir))\n",
    "#     save_path = os.path.join(model_dir, model_checkpoint.replace('/', '-'))\n",
    "#     torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PyTorch Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # PyTorch dataset retrieves the dataset’s features and labels one sample at a time\n",
    "# # Create a custom Dataset class for the reviews\n",
    "# class ReviewDataset(Dataset):\n",
    "    \n",
    "#     def __init__(self, input_ids_list, label_id_list):\n",
    "#         self.input_ids_list = input_ids_list\n",
    "#         self.label_id_list = label_id_list\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.input_ids_list)\n",
    "\n",
    "#     def __getitem__(self, item):\n",
    "#         # convert list of token_ids into an array of PyTorch LongTensors\n",
    "#         input_ids = json.loads(self.input_ids_list[item]) \n",
    "#         label_id = self.label_id_list[item]\n",
    "\n",
    "#         input_ids_tensor = torch.LongTensor(input_ids)\n",
    "#         label_id_tensor = torch.tensor(label_id, dtype=torch.long)\n",
    "\n",
    "#         return input_ids_tensor, label_id_tensor\n",
    "\n",
    "    \n",
    "# # PyTorch DataLoader helps to to organise the input training data in “minibatches” and reshuffle the data at every epoch\n",
    "# # It takes Dataset as an input\n",
    "# def create_data_loader(path, batch_size): \n",
    "#     print(\"Get data loader\")\n",
    "\n",
    "#     df = pd.DataFrame(columns=['input_ids', 'label_id'])\n",
    "    \n",
    "#     input_files = create_list_input_files(path)\n",
    "\n",
    "#     for file in input_files:\n",
    "#         # df_temp = pd.read_csv(file, \n",
    "#         #                       sep='\\t', \n",
    "#         #                       usecols=['input_ids', 'label_id'])\n",
    "#         df_temp = pd.read_parquet(file)\n",
    "#         df = df.append(df_temp)\n",
    "#         print('adding df_temp: {}'.format(df_temp))\n",
    "        \n",
    "#     ds = ReviewDataset(\n",
    "#         input_ids_list=df.input_ids.to_numpy(),\n",
    "#         label_id_list=df.label_id.to_numpy(),\n",
    "#     )\n",
    "    \n",
    "#     return DataLoader(\n",
    "#         ds,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True,\n",
    "#         drop_last=True,\n",
    "#     ), df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # TODO:  Change this to binary classification\n",
    "# #        where 1 is assigned to the human-selected (presumably-correct) label\n",
    "# #        and 0 is assigned to all of other labels\n",
    "\n",
    "# def get_model_config():\n",
    "#     classes = [1, 2, 3, 4, 5]\n",
    "\n",
    "#     config = AutoConfig.from_pretrained(\n",
    "#         supervised_fine_tuned_model_path,        \n",
    "#         num_labels=len(classes),\n",
    "#         id2label={\n",
    "#             0: 1, \n",
    "#             1: 2, \n",
    "#             2: 3, \n",
    "#             3: 4, \n",
    "#             4: 5            \n",
    "#         },\n",
    "#         label2id={\n",
    "#             1: 0,\n",
    "#             2: 1,\n",
    "#             3: 2,\n",
    "#             4: 3,\n",
    "#             5: 4\n",
    "#         }\n",
    "#     )\n",
    "    \n",
    "#     config.output_attentions=True\n",
    "\n",
    "#     return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def train_model(model,\n",
    "#                 train_data_loader,\n",
    "#                 df_train,\n",
    "#                 val_data_loader, \n",
    "#                 df_val,\n",
    "#                 args):\n",
    "    \n",
    "#     loss_function = nn.CrossEntropyLoss()    \n",
    "#     optimizer = optim.Adam(params=model.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "#     if args.freeze_base_layers:\n",
    "#         print('Freezing base layers...')\n",
    "#         for name, param in model.named_parameters():\n",
    "#             if 'classifier' not in name:  # classifier layer\n",
    "#                 param.requires_grad = False\n",
    "#         print('Set classifier layers to `param.requires_grad=False`.')        \n",
    "    \n",
    "#     train_correct = 0\n",
    "#     train_total = 0\n",
    "\n",
    "#     for epoch in range(args.epochs):\n",
    "#         print('EPOCH -- {}'.format(epoch))\n",
    "\n",
    "#         for i, (sent, label) in enumerate(train_data_loader):\n",
    "#             print('i: ' + i)\n",
    "#             print('sent: ' + sent)\n",
    "#             print('label: ' + label)            \n",
    "#             if i < args.train_steps_per_epoch:\n",
    "#                 model.train()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 sent = sent.squeeze(0)\n",
    "#                 if torch.cuda.is_available():\n",
    "#                     sent = sent.cuda()\n",
    "#                     label = label.cuda()\n",
    "#                 output = model(sent)[0]\n",
    "#                 _, predicted = torch.max(output, 1)\n",
    "\n",
    "#                 loss = loss_function(output, label)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "            \n",
    "#                 if args.run_validation and i % args.validation_steps_per_epoch == 0:\n",
    "#                     print('RUNNING VALIDATION:')\n",
    "#                     correct = 0\n",
    "#                     total = 0\n",
    "#                     model.eval()\n",
    "\n",
    "#                     for sent, label in val_data_loader:\n",
    "#                         sent = sent.squeeze(0)\n",
    "#                         if torch.cuda.is_available():\n",
    "#                             sent = sent.cuda()\n",
    "#                             label = label.cuda()\n",
    "#                         output = model(sent)[0]\n",
    "#                         _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "#                         total += label.size(0)\n",
    "#                         correct += (predicted.cpu() ==label.cpu()).sum()\n",
    "\n",
    "#                     accuracy = 100.00 * correct.numpy() / total\n",
    "#                     print('[epoch/step: {0}/{1}] val_loss: {2:.2f} - val_acc: {3:.2f}%'.format(epoch, i, loss.item(), accuracy))\n",
    "#             else:\n",
    "#                 break           \n",
    "\n",
    "#     print('TRAINING COMPLETED.')\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# import random\n",
    "\n",
    "# #if __name__ == '__main__':\n",
    "    \n",
    "# # Parse args\n",
    "\n",
    "\n",
    "# os.environ['SM_HOSTS'] = '{\"hosts\": [\"algo-1\"]}'\n",
    "# os.environ['SM_CURRENT_HOST'] = 'algo-1'\n",
    "# os.environ['SM_NUM_GPUS'] = '0'\n",
    "# os.environ['SM_MODEL_DIR'] = './model/reward_model/'\n",
    "# os.environ['SM_CHANNEL_TRAIN'] = './data/train'\n",
    "# os.environ['SM_CHANNEL_VALIDATION'] = './data/validation'\n",
    "# os.environ['SM_OUTPUT_DIR'] = './model_output/'\n",
    "\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "\n",
    "# # CLI args\n",
    "\n",
    "# parser.add_argument('--train_batch_size', \n",
    "#                     type=int, \n",
    "#                     default=64)\n",
    "\n",
    "# parser.add_argument('--train_steps_per_epoch',\n",
    "#                     type=int,\n",
    "#                     default=64)\n",
    "\n",
    "# parser.add_argument('--validation_batch_size', \n",
    "#                     type=int, \n",
    "#                     default=64)\n",
    "\n",
    "# parser.add_argument('--validation_steps_per_epoch',\n",
    "#                     type=int,\n",
    "#                     default=64)\n",
    "\n",
    "# parser.add_argument('--epochs', \n",
    "#                     type=int, \n",
    "#                     default=10)\n",
    "\n",
    "# parser.add_argument('--freeze_base_layers', \n",
    "#                     type=eval, \n",
    "#                     default=False)\n",
    "\n",
    "# parser.add_argument('--learning_rate', \n",
    "#                     type=float, \n",
    "#                     default=0.01)\n",
    "\n",
    "# parser.add_argument('--momentum', \n",
    "#                     type=float, \n",
    "#                     default=0.5)\n",
    "\n",
    "# parser.add_argument('--seed', \n",
    "#                     type=int, \n",
    "#                     default=42)\n",
    "\n",
    "# parser.add_argument('--log_interval', \n",
    "#                     type=int, \n",
    "#                     default=100)\n",
    "\n",
    "# parser.add_argument('--backend', \n",
    "#                     type=str, \n",
    "#                     default=None)\n",
    "\n",
    "# parser.add_argument('--run_validation', \n",
    "#                     type=eval,\n",
    "#                     default=False)\n",
    "\n",
    "# parser.add_argument('--model-checkpoint', \n",
    "#                     type=str,\n",
    "#                     default=model_checkpoint)\n",
    "\n",
    "\n",
    "# # Container environment  \n",
    "\n",
    "# parser.add_argument('--hosts', \n",
    "#                     type=list, \n",
    "#                     default=json.loads(os.environ['SM_HOSTS']))\n",
    "\n",
    "# parser.add_argument('--current_host', \n",
    "#                     type=str, \n",
    "#                     default=os.environ['SM_CURRENT_HOST'])\n",
    "\n",
    "# parser.add_argument('--model_dir', \n",
    "#                     type=str, \n",
    "#                     default=os.environ['SM_MODEL_DIR'])\n",
    "\n",
    "# parser.add_argument('--train_data', \n",
    "#                     type=str, \n",
    "#                     default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "# parser.add_argument('--validation_data', \n",
    "#                     type=str, \n",
    "#                     default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "\n",
    "# parser.add_argument('--output_dir', \n",
    "#                     type=str, \n",
    "#                     default=os.environ['SM_OUTPUT_DIR'])\n",
    "\n",
    "# parser.add_argument('--num_gpus', \n",
    "#                     type=int, \n",
    "#                     default=os.environ['SM_NUM_GPUS'])\n",
    "\n",
    "# # Debugger args\n",
    "\n",
    "# parser.add_argument(\"--save-frequency\", \n",
    "#                     type=int, \n",
    "#                     default=10, \n",
    "#                     help=\"frequency with which to save steps\")\n",
    "\n",
    "# parser.add_argument(\"--smdebug_path\",\n",
    "#                     type=str,\n",
    "#                     help=\"output directory to save data in\",\n",
    "#                     default=\"/opt/ml/output/tensors\",)\n",
    "\n",
    "# parser.add_argument(\"--hook-type\",\n",
    "#                     type=str,\n",
    "#                     choices=[\"saveall\", \"module-input-output\", \"weights-bias-gradients\"],\n",
    "#                     default=\"saveall\",)\n",
    "\n",
    "# args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "# print('Loaded arguments:')\n",
    "# print(args)\n",
    "\n",
    "# # Get environment variables\n",
    "\n",
    "# env_var = os.environ \n",
    "# print('Environment variables:')\n",
    "# pprint(dict(env_var), width = 1) \n",
    "\n",
    "# # Check if distributed training\n",
    "\n",
    "# is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "\n",
    "# print(\"Distributed training - {}\".format(is_distributed))\n",
    "# use_cuda = args.num_gpus > 0\n",
    "# print(\"Number of gpus available - {}\".format(args.num_gpus))\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "# device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# # Initialize the distributed environment.\n",
    "\n",
    "# if is_distributed:\n",
    "#     world_size = len(args.hosts)\n",
    "#     os.environ['WORLD_SIZE'] = str(world_size)\n",
    "#     host_rank = args.hosts.index(args.current_host)\n",
    "#     os.environ['RANK'] = str(host_rank)\n",
    "#     dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
    "#     print('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n",
    "#         args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
    "#         dist.get_rank(), args.num_gpus))\n",
    "\n",
    "# # Set the seed for generating random numbers\n",
    "\n",
    "# torch.manual_seed(args.seed)\n",
    "# if use_cuda:\n",
    "#     torch.cuda.manual_seed(args.seed) \n",
    "\n",
    "# # Instantiate model\n",
    "\n",
    "# config = None\n",
    "# model = None\n",
    "\n",
    "# successful_download = False\n",
    "# retries = 0\n",
    "\n",
    "# while (retries < 5 and not successful_download):\n",
    "#     try:\n",
    "#         # Setup model\n",
    "#         config = get_model_config()\n",
    "#         model = AutoModelForSequenceClassification.from_pretrained(\n",
    "#             supervised_fine_tuned_model_path,\n",
    "#             config=config\n",
    "#         )\n",
    "\n",
    "#         model.to(device)\n",
    "#         successful_download = True\n",
    "#         print('Sucessfully downloaded after {} retries.'.format(retries))\n",
    "\n",
    "#     except:\n",
    "#         retries = retries + 1\n",
    "#         random_sleep = random.randint(1, 30)\n",
    "#         print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\n",
    "#         time.sleep(random_sleep)\n",
    "\n",
    "# if not model:\n",
    "#      print('Not properly initialized...')\n",
    "\n",
    "# # Create data loaders\n",
    "\n",
    "# train_data_loader, df_train = create_data_loader(args.train_data, args.train_batch_size)\n",
    "# val_data_loader, df_val = create_data_loader(args.validation_data, args.validation_batch_size)\n",
    "\n",
    "# print(\"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "#     len(train_data_loader.sampler), len(train_data_loader.dataset),\n",
    "#     100. * len(train_data_loader.sampler) / len(train_data_loader.dataset)\n",
    "# ))\n",
    "\n",
    "# print(\"Processes {}/{} ({:.0f}%) of validation data\".format(\n",
    "#     len(val_data_loader.sampler), len(val_data_loader.dataset),\n",
    "#     100. * len(val_data_loader.sampler) / len(val_data_loader.dataset)\n",
    "# )) \n",
    "\n",
    "# print('model_dir: {}'.format(args.model_dir))    \n",
    "# #print('model summary: {}'.format(model))\n",
    "\n",
    "# callbacks = []\n",
    "# initial_epoch_number = 0\n",
    "\n",
    "# # Start training\n",
    "\n",
    "# model = train_model(\n",
    "#     model,\n",
    "#     train_data_loader,\n",
    "#     df_train,\n",
    "#     val_data_loader, \n",
    "#     df_val,\n",
    "#     args\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_transformer_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-9a54be383e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msave_transformer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_pytorch_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_transformer_model' is not defined"
     ]
    }
   ],
   "source": [
    "save_transformer_model(model, args.model_dir)\n",
    "save_pytorch_model(model, args.model_checkpoint, args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "reward_model_path = os.path.join(args.model_dir, 'transformer'),\n",
    "\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "inference_pipeline = pipeline(\"text-classification\", \n",
    "                              reward_model_path,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "\n",
    "# <p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "# <button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "# <script>\n",
    "# try {\n",
    "#     els = document.getElementsByClassName(\"sm-command-button\");\n",
    "#     els[0].click();\n",
    "# }\n",
    "# catch(err) {\n",
    "#     // NoOp\n",
    "# }    \n",
    "# </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.p3dn.24xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
