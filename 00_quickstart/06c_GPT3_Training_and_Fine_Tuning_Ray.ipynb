{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c55d7a43-511f-43e8-8fcb-ce0942daf400",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.13.1\n",
      "Uninstalling torch-1.13.1:\n",
      "  Successfully uninstalled torch-1.13.1\n",
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mFound existing installation: torchdata 0.5.1\n",
      "Uninstalling torchdata-0.5.1:\n",
      "  Successfully uninstalled torchdata-0.5.1\n",
      "\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting torch==1.13.1\n",
      "  Using cached torch-1.13.1-cp38-cp38-manylinux1_x86_64.whl (887.4 MB)\n",
      "Collecting torchdata\n",
      "  Using cached torchdata-0.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch==1.13.1) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.8/site-packages (from torch==1.13.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.8/site-packages (from torch==1.13.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.8/site-packages (from torch==1.13.1) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (65.6.3)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.8/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1) (0.38.4)\n",
      "Requirement already satisfied: portalocker>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from torchdata) (2.6.0)\n",
      "Requirement already satisfied: urllib3>=1.25 in /opt/conda/lib/python3.8/site-packages (from torchdata) (1.26.13)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchdata) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.8/site-packages (from requests->torchdata) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchdata) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchdata) (2022.12.7)\n",
      "Installing collected packages: torch, torchdata\n",
      "Successfully installed torch-1.13.1 torchdata-0.5.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Found existing installation: ray 2.2.0\n",
      "Uninstalling ray-2.2.0:\n",
      "  Successfully uninstalled ray-2.2.0\n",
      "Found existing installation: transformers 4.26.1\n",
      "Uninstalling transformers-4.26.1:\n",
      "  Successfully uninstalled transformers-4.26.1\n",
      "Found existing installation: datasets 2.9.0\n",
      "Uninstalling datasets-2.9.0:\n",
      "  Successfully uninstalled datasets-2.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.26.1-py3-none-any.whl (6.3 MB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.9.0-py3-none-any.whl (462 kB)\n",
      "Collecting ray==2.2.0\n",
      "  Using cached ray-2.2.0-cp38-cp38-manylinux2014_x86_64.whl (57.4 MB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (2.28.1)\n",
      "Requirement already satisfied: virtualenv>=20.0.24 in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (20.19.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (1.0.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (5.4.1)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (22.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (8.1.3)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (1.23.5)\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (1.3.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (3.9.0)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (3.19.6)\n",
      "Requirement already satisfied: grpcio>=1.32.0 in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (1.51.1)\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (1.3.1)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.8/site-packages (from ray==2.2.0) (4.17.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.12.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /opt/conda/lib/python3.8/site-packages (from datasets) (10.0.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.8/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.8/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.8/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from datasets) (1.5.2)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.8/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.8/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->ray==2.2.0) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->ray==2.2.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->ray==2.2.0) (2022.12.7)\n",
      "Requirement already satisfied: platformdirs<4,>=2.4 in /opt/conda/lib/python3.8/site-packages (from virtualenv>=20.0.24->ray==2.2.0) (3.0.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from virtualenv>=20.0.24->ray==2.2.0) (0.3.6)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema->ray==2.2.0) (0.19.3)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /opt/conda/lib/python3.8/site-packages (from jsonschema->ray==2.2.0) (1.3.10)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from jsonschema->ray==2.2.0) (5.10.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema->ray==2.2.0) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: transformers, ray, datasets\n",
      "Successfully installed datasets-2.9.0 ray-2.2.0 transformers-4.26.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall -y torch torchvision torchdata torchaudio\n",
    "%pip install -U torch==1.13.1 torchdata\n",
    "\n",
    "%pip uninstall -y ray transformers datasets\n",
    "%pip install -U transformers datasets ray==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edb982d-60c0-4fbf-b011-6da0903f7cc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-pytorch-training 2.7.0\n",
      "torch                      1.13.1\n",
      "torchdata                  0.5.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e175b6b0-81e2-4256-8914-b520a8cb254c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel to pick up the pip installs above\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True) #automatically restarts kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c75863ba-548f-40d2-b2ce-048bbfecfba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "\n",
    "#os.environ[\"PL_TORCH_DISTRIBUTED_BACKEND\"] = \"gloo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c71014b-40ea-4748-9f25-2096f0b2cd79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset amazon_us_reviews (/root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563)\n",
      "100%|██████████| 1/1 [00:00<00:00, 189.52it/s]\n",
      "Found cached dataset amazon_us_reviews (/root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Software_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563)\n",
      "100%|██████████| 1/1 [00:00<00:00, 232.75it/s]\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-f73925383eaf671a.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-499b7cc12b6587c8.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-c7d5f9f46dddbc82.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-f3c606beedb9845f.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Software_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-66c02d403675dd31.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Software_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-03e96ffa6a90120c.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Software_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-34a139ccb039bc18.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Software_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-feb1c7c550f1c0ee.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-27c44c05608e5584.arrow\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_us_reviews/Digital_Software_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563/cache-1cdbbf16fea4bfcc.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [121605, 80578], 'attention_mask': [1, 1]}\n",
      "{'input_ids': [19405, 25689, 267, 10512, 3172, 2909, 37663], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "#model_checkpoint = \"gpt2\"\n",
    "\n",
    "model_checkpoint = \"bigscience/bloom-560m\"\n",
    "\n",
    "#tokenizer_checkpoint = \"sgugger/gpt2-like-tokenizer\"\n",
    "block_size = 128\n",
    "\n",
    "#datasets = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "\n",
    "dataset_train = load_dataset(\"amazon_us_reviews\", \"Digital_Video_Games_v1_00\") #, data_files={\"train\": \"Apparel_v1_00\"})  #, \"validation\": path_to_validation.txt}\n",
    "\n",
    "dataset_validation = load_dataset(\"amazon_us_reviews\", \"Digital_Software_v1_00\") #, data_files={\"train\": \"Apparel_v1_00\"})  #, \"validation\": path_to_validation.txt}\n",
    "\n",
    "# data_files = {}\n",
    "# data_files[\"train\"] = f\"{data_path}/test/part-algo-1-womens_clothing_ecommerce_reviews.csv\"\n",
    "# extension = \"csv\"\n",
    "\n",
    "# raw_datasets = load_dataset(extension, data_files=data_files)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "text_column_name = \"review_body\"\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(examples[text_column_name])\n",
    "    return tokenized\n",
    "    \n",
    "tokenized_dataset_train = dataset_train.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\n",
    "    'marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', \n",
    "    'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', \n",
    "    'review_headline', 'review_date', text_column_name])\n",
    "\n",
    "tokenized_dataset_validation = dataset_validation.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\n",
    "    'marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', \n",
    "    'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', \n",
    "    'review_headline', 'review_date', text_column_name])\n",
    "\n",
    "print(tokenized_dataset_train[\"train\"][1])\n",
    "print(tokenized_dataset_validation[\"train\"][1])\n",
    "\n",
    "block_size = 128\n",
    "\n",
    "# tokenized_datasets = datasets.map(\n",
    "#     tokenize_function, batched=True, num_proc=1, remove_columns=[\"text\"]\n",
    "# )\n",
    "\n",
    "\n",
    "def group_texts(examples):    \n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "        # customize this part to your needs.\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "\n",
    "lm_dataset_train = tokenized_dataset_train.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "lm_dataset_validation = tokenized_dataset_validation.map(\n",
    "    group_texts,\n",
    "    batched=True,\n",
    "    batch_size=1000,\n",
    "    num_proc=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6273fcb-886c-4f57-ab02-b78822e6181c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-15 21:02:20,906\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "2023-02-15 21:02:22,648\tINFO data_parallel_trainer.py:286 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "/opt/conda/lib/python3.8/site-packages/ray/train/base_trainer.py:354: UserWarning: Executing `.fit()` may leave less than 20% of CPUs in this cluster for Dataset execution, which can lead to resource contention or hangs. To avoid this, reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer scaling_config. See https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune for more info.\n",
      "  tuner = Tuner(trainable=trainable, run_config=self.run_config)\n",
      "2023-02-15 21:02:22,727\tINFO tensorboardx.py:170 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-02-15 21:02:22,728\tWARNING callback.py:108 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-02-15 21:48:18</td></tr>\n",
       "<tr><td>Running for: </td><td>00:45:55.53        </td></tr>\n",
       "<tr><td>Memory:      </td><td>16.0/124.6 GiB     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 31.0/32 CPUs, 0/1 GPUs, 0.0/95.19 GiB heap, 0.0/14.73 GiB objects (0.0/1.0 accelerator_type:A10G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">      epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_0acb7_00000</td><td>RUNNING </td><td>169.255.254.2:14576</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1598.64</td><td style=\"text-align: right;\">4.0161</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">0.000860882</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HuggingFaceTrainer pid=14576)\u001b[0m 2023-02-15 21:02:25,728\tINFO data_parallel_trainer.py:286 -- GPUs are detected in your Ray cluster, but GPU training is not enabled for this trainer. To enable GPU training, make sure to set `use_gpu` to True in your scaling config.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m 2023-02-15 21:02:27,725\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m max_steps is given, it will override any value given in num_train_epochs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m /opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m ***** Running training *****\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   Num examples = 92921\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   Num Epochs = 1\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   Instantaneous batch size per device = 8\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   Gradient Accumulation steps = 1\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   Total optimization steps = 10\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m   Number of trainable parameters = 559214592\n",
      "  0%|          | 0/10 [00:00<?, ?it/s]m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m [W reducer.cpp:1298] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      " 10%|█         | 1/10 [02:35<23:18, 155.43s/it]\n",
      " 20%|██        | 2/10 [05:09<20:36, 154.51s/it]\n",
      " 30%|███       | 3/10 [07:42<17:58, 154.12s/it]\n",
      " 40%|████      | 4/10 [10:15<15:19, 153.31s/it]\n",
      " 50%|█████     | 5/10 [12:48<12:46, 153.27s/it]\n",
      " 60%|██████    | 6/10 [15:21<10:13, 153.37s/it]\n",
      " 70%|███████   | 7/10 [17:54<07:39, 153.08s/it]\n",
      " 80%|████████  | 8/10 [20:26<05:05, 152.68s/it]\n",
      " 90%|█████████ | 9/10 [22:58<02:32, 152.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m {'loss': 4.0161, 'learning_rate': 0.0, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [25:30<00:00, 152.52s/it]Saving model checkpoint to bigscience/bloom-560m-finetuned-amazon-customer-reviews/checkpoint-10\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m Configuration saved in bigscience/bloom-560m-finetuned-amazon-customer-reviews/checkpoint-10/config.json\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m Configuration saved in bigscience/bloom-560m-finetuned-amazon-customer-reviews/checkpoint-10/generation_config.json\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m Model weights saved in bigscience/bloom-560m-finetuned-amazon-customer-reviews/checkpoint-10/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m {'train_runtime': 1588.9661, 'train_samples_per_second': 0.05, 'train_steps_per_second': 0.006, 'train_loss': 4.016141891479492, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=14712)\u001b[0m \n",
      "100%|██████████| 10/10 [26:28<00:00, 158.90s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  _time_this_iter_s</th><th style=\"text-align: right;\">  _timestamp</th><th style=\"text-align: right;\">  _training_iteration</th><th>date               </th><th>done  </th><th>episodes_total  </th><th style=\"text-align: right;\">      epoch</th><th>experiment_id                   </th><th>hostname                                                       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  loss</th><th>node_ip      </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_runtime</th><th style=\"text-align: right;\">  train_samples_per_second</th><th style=\"text-align: right;\">  train_steps_per_second</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_0acb7_00000</td><td style=\"text-align: right;\">            1595.64</td><td style=\"text-align: right;\">  1676496544</td><td style=\"text-align: right;\">                    1</td><td>2023-02-15_21-29-04</td><td>False </td><td>                </td><td style=\"text-align: right;\">0.000860882</td><td>c58abde027b646adb64519e3aa670960</td><td>pytorch-1-12-cpu-py3-ml-g5-8xlarge-dadc2efa7892c7009794b397aac6</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">              0</td><td style=\"text-align: right;\">4.0161</td><td>169.255.254.2</td><td style=\"text-align: right;\">14576</td><td>True               </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">             1598.64</td><td style=\"text-align: right;\">           1598.64</td><td style=\"text-align: right;\">       1598.64</td><td style=\"text-align: right;\"> 1676496544</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">     4.01614</td><td style=\"text-align: right;\">        1588.97</td><td style=\"text-align: right;\">                      0.05</td><td style=\"text-align: right;\">                   0.006</td><td style=\"text-align: right;\">                   1</td><td>0acb7_00000</td><td style=\"text-align: right;\">   0.00494528</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ray\n",
    "ray.shutdown()\n",
    "\n",
    "from ray.train.huggingface import HuggingFaceTrainer\n",
    "from ray.air.config import ScalingConfig, RunConfig\n",
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n",
    "from ray.tune import SyncConfig\n",
    "\n",
    "\n",
    "# ray.init(runtime_env={\"pip\": [\n",
    "#                         \"torch\", \n",
    "#                         \"scikit-learn\",\n",
    "#                         \"transformers\",\n",
    "#                         \"pandas\",\n",
    "#                         \"datasets\",\n",
    "#                         \"accelerate\",\n",
    "#                         \"scikit-learn\",\n",
    "#                         \"mlflow\", \n",
    "#                         \"tensorboard\",\n",
    "#                         \"s3fs\"]\n",
    "#                      }\n",
    "#         )\n",
    "\n",
    "def trainer_init_per_worker(train_dataset, \n",
    "                            eval_dataset, \n",
    "                            **config):\n",
    "    #model_config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "    #model = AutoModelForCausalLM.from_config(model_config)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"{model_checkpoint}-finetuned-amazon-customer-reviews\",\n",
    "#        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,  \n",
    "        max_steps=10,\n",
    "#        eval_steps=10,\n",
    "        num_train_epochs=1.0,\n",
    "        no_cuda=True        \n",
    "    )\n",
    "    \n",
    "    return Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "#        eval_dataset=eval_dataset,\n",
    "    )\n",
    "\n",
    "\n",
    "ray_train_ds = ray.data.from_huggingface(\n",
    "    lm_dataset_train[\"train\"]\n",
    ")\n",
    "\n",
    "ray_evaluation_ds = ray.data.from_huggingface(\n",
    "    lm_dataset_validation[\"train\"]\n",
    ")\n",
    "\n",
    "\n",
    "s3_checkpoint_prefix=\"s3://dsoaws/ray_output\"\n",
    "\n",
    "import multiprocessing\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=1, \n",
    "                               #use_gpu=True,\n",
    "                               trainer_resources={\"CPU\": num_cpus - 2}, #, \"GPU\": 0},\n",
    "                               #_max_cpu_fraction_per_node = 0.8\n",
    "                              )\n",
    "\n",
    "# If using GPUs, use the below scaling config instead.\n",
    "# scaling_config = ScalingConfig(num_workers=3, use_gpu=True)\n",
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config = RunConfig(\n",
    "        sync_config=SyncConfig(\n",
    "            # This will store checkpoints in S3.\n",
    "            upload_dir=s3_checkpoint_prefix\n",
    "        )\n",
    "    ),\n",
    "    datasets={\"train\": ray_train_ds, \n",
    "              #\"evaluation\": ray_evaluation_ds\n",
    "             },\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb292b81-f621-4601-bf90-0644c5cb299b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42662f4-3ea2-42d8-958f-b31f4ba81478",
   "metadata": {},
   "outputs": [],
   "source": [
    "TODO.save_pretrained('s3://dsoaws/bloom/output/model_from_notebook_ray/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379679db-65c4-4e12-af87-94e3f6d7a1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bigscience/bloom-560m', use_fast=True)\n",
    "#model = AutoModelForCausalLM.from_pretrained('s3://dsoaws/bloom/output/model_from_notebook_ray/')\n",
    "                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4a201-2a28-4c2d-ab8b-c4e763d2ffa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(trainer.trainer_init_per_worker.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f327a1b-36da-48c0-970f-f84c3b6a0ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"Norton Antivirus\"\n",
    "result_length = 100\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d39171-da7d-4e1b-a278-911909d59508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"],\n",
    "                       max_length=result_length, \n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05132350-14a0-466e-a191-d1a8ee4f09e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
