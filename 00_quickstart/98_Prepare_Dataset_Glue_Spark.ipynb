{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd758f9b-126f-45ff-a124-94f83d41126f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker + Glue Interactive Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f759e14-cbe3-416f-9e06-833e4dcadef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the Glue Interactive Sessions Kernel\n",
      "For more information on available magic commands, please type %help in any new cell.\n",
      "\n",
      "Please view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\n",
      "It looks like there is a newer version of the kernel available. The latest version is 0.37.2 and you have 0.37.0 installed.\n",
      "Please run `pip install --upgrade aws-glue-sessions` to upgrade your kernel\n",
      "There is no current session.\n"
     ]
    }
   ],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08363097-99f3-4aa8-9ce4-a937727c979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional python modules to be included:\n",
      "transformers\n",
      "datasets\n",
      "torch\n",
      "Previous number of workers: 10\n",
      "Setting new number of workers to: 10\n"
     ]
    }
   ],
   "source": [
    "%additional_python_modules transformers,torch\n",
    "%number_of_workers 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cb67dd-a85e-4a16-a97c-01d562de6460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::079002598131:role/service-role/AmazonSageMaker-ExecutionRole-20220804T150518\n",
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 10\n",
      "Session ID: c78ff64c-28ab-409d-87b9-f53812c8bff4\n",
      "Job Type: glueetl\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.37.0\n",
      "--enable-glue-datacatalog true\n",
      "--additional-python-modules transformers,datasets,torch\n",
      "Waiting for session c78ff64c-28ab-409d-87b9-f53812c8bff4 to get into ready status...\n",
      "Session c78ff64c-28ab-409d-87b9-f53812c8bff4 has been created.\n",
      "3.7.16 (default, Dec 15 2022, 23:24:54) \n",
      "[GCC 7.3.1 20180712 (Red Hat 7.3.1-15)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e1b6a1b-0429-4fb9-9024-62c970ce79d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers, torch #, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f518717-84cb-4ae2-86b7-6092abf8ef7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset amazon_us_reviews/Digital_Video_Games_v1_00 to /home/spark/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563...\n",
      "Dataset amazon_us_reviews downloaded and prepared to /home/spark/.cache/huggingface/datasets/amazon_us_reviews/Digital_Video_Games_v1_00/0.1.0/17b2481be59723469538adeb8fd0a68b0ba363bbbdd71090e72c325ee6c7e563. Subsequent calls will reuse this data.\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'star_rating', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_body', 'review_date'],\n",
      "        num_rows: 145431\n",
      "    })\n",
      "})\n",
      "Downloading builder script: 100%|##########| 7.45k/7.45k [00:00<00:00, 7.84MB/s]\n",
      "Downloading metadata: 100%|##########| 195k/195k [00:00<00:00, 47.4MB/s]\n",
      "Downloading readme: 100%|##########| 60.0k/60.0k [00:00<00:00, 30.2MB/s]\n",
      "Downloading data: 100%|##########| 27.4M/27.4M [00:00<00:00, 58.7MB/s]\n",
      "100%|##########| 1/1 [00:00<00:00, 192.75it/s]                                          \n"
     ]
    }
   ],
   "source": [
    "# from datasets import load_dataset\n",
    "    \n",
    "# datasets = load_dataset(\"amazon_us_reviews\", \"Digital_Video_Games_v1_00\") #, data_files={\"train\": \"Apparel_v1_00\"})  #, \"validation\": path_to_validation.txt}\n",
    "\n",
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc228da-a391-4044-bd42-943ef0fa2d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet(\"s3://dsoaws/parquet/\")\n",
    "\n",
    "# The following command caches the DataFrame in memory. This improves performance since subsequent calls to the DataFrame can read from memory instead of re-reading the data from disk.\n",
    "#df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e8011d0-3b89-4450-bf25-37626ab07a44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE CFN_TEMPLATES_IS_USED_BY_DEEPLEARNING_AI_DO_NOT_DELETE/\n",
      "                           PRE archive/\n",
      "                           PRE autopilot/\n",
      "                           PRE cfn_templates/\n",
      "                           PRE cfn_templates_canvas/\n",
      "                           PRE cfn_templates_hands_on/\n",
      "                           PRE content-moderation/\n",
      "                           PRE data-tfrecord-all/\n",
      "                           PRE data/\n",
      "                           PRE dataSharing/\n",
      "                           PRE emr-studio/\n",
      "                           PRE emr/\n",
      "                           PRE feast/\n",
      "                           PRE nyc-taxi-orig-cleaned-csv-with-header-all-years/\n",
      "                           PRE nyc-taxi-orig-cleaned-csv-with-header-per-year/\n",
      "                           PRE nyc-taxi-orig-cleaned-csv-without-header-all-years/\n",
      "                           PRE nyc-taxi-orig-cleaned-csv-without-header-per-year/\n",
      "                           PRE nyc-taxi-orig-cleaned-parquet-2019-under-1gb/\n",
      "                           PRE nyc-taxi-orig-cleaned-parquet-all-years-multiple-files/\n",
      "                           PRE nyc-taxi-orig-cleaned-parquet-all-years/\n",
      "                           PRE nyc-taxi-orig-cleaned-parquet-per-year-multiple-files/\n",
      "                           PRE nyc-taxi-orig-cleaned-parquet-per-year/\n",
      "                           PRE nyc-taxi-orig-cleaned-parquet-uncompressed-2019-only/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-csv-with-header-all-years/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-csv-with-header-per-year-multiple-files/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-csv-with-header-per-year/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-csv-without-header-all-years/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-csv-without-header-per-year/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-parquet-all-years-multiple-files/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-parquet-all-years/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-parquet-per-year-multiple-files/\n",
      "                           PRE nyc-taxi-orig-cleaned-split-parquet-per-year/\n",
      "                           PRE parquet/\n",
      "                           PRE ray/\n",
      "                           PRE ray_output/\n",
      "                           PRE spark-gis-event-logs/\n",
      "                           PRE tmp/\n",
      "                           PRE tsv/\n",
      "                           PRE ux360/\n",
      "                           PRE videos/\n",
      "                           PRE workshop/\n",
      "2022-05-15 02:57:45  845066692 model.tar.gz\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28c4c3e4-d2ec-4704-84d4-8b0704f67d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------+-----------+-----+-------+----------+------------+----+\n",
      "|total_amount|      ride_id|vendor_id|          pickup_at|         dropoff_at|passenger_count|trip_distance|rate_code_id|store_and_fwd_flag|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|year|\n",
      "+------------+-------------+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------+-----------+-----+-------+----------+------------+----+\n",
      "|        17.8|3049427049247|        1|2017-08-19 02:09:25|2017-08-19 02:23:43|              1|          4.2|           1|                 N|           1|       15.0|  0.5|    0.5|       1.5|         0.0|2017|\n",
      "|       12.36|3049427049248|        2|2017-01-23 11:26:13|2017-01-23 11:39:54|              1|         1.31|           1|                 N|           1|        9.5|  0.0|    0.5|      2.06|         0.0|2017|\n",
      "|       22.55|3049427049249|        1|2017-08-19 02:35:42|2017-08-19 02:51:13|              1|          5.5|           1|                 N|           1|       17.5|  0.5|    0.5|      3.75|         0.0|2017|\n",
      "|        24.3|3049427049250|        2|2017-01-23 11:26:13|2017-01-23 11:48:18|              5|         7.17|           1|                 N|           2|       23.5|  0.0|    0.5|       0.0|         0.0|2017|\n",
      "|       52.87|3049427049251|        1|2017-08-19 02:26:29|2017-08-19 02:28:37|              1|          0.0|           2|                 N|           1|       52.0|  0.0|    0.5|      0.07|         0.0|2017|\n",
      "|         5.8|3049427049252|        2|2017-01-23 11:26:13|2017-01-23 11:30:47|              5|         0.64|           1|                 N|           2|        5.0|  0.0|    0.5|       0.0|         0.0|2017|\n",
      "|       44.75|3049427049253|        2|2017-08-19 02:03:52|2017-08-19 02:35:08|              1|        11.45|           1|                 N|           1|       34.5|  0.5|    0.5|      8.95|         0.0|2017|\n",
      "|         4.3|3049427049254|        2|2017-01-23 11:26:13|2017-01-23 11:28:24|              2|         0.44|           1|                 N|           2|        3.5|  0.0|    0.5|       0.0|         0.0|2017|\n",
      "|        18.8|3049427049255|        1|2017-08-19 02:24:36|2017-08-19 02:42:09|              2|          4.9|           1|                 N|           2|       17.5|  0.5|    0.5|       0.0|         0.0|2017|\n",
      "|       45.99|3049427049256|        1|2017-01-23 11:26:14|2017-01-23 11:50:41|              1|         10.8|           1|                 N|           1|       32.0|  0.0|    0.5|      7.65|        5.54|2017|\n",
      "+------------+-------------+---------+-------------------+-------------------+---------------+-------------+------------+------------------+------------+-----------+-----+-------+----------+------------+----+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad669ca1-d8ce-4c02-b4a8-0986bb1effb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1054038765 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset has %d rows.\" % data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec3b8b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 1054038765 rows.\n"
     ]
    }
   ],
   "source": [
    "print(\"The dataset has %d rows.\" % data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df10a15b-6d3d-495f-baab-24decc917d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_amount_by_passenger_count = data.select('passenger_count', 'total_amount') \\\n",
    "                                     .groupby('passenger_count') \\\n",
    "                                     .avg('total_amount') \\\n",
    "                                     .sort('passenger_count')\n",
    "df_avg_amount_by_passenger_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6944f3d5-7c69-43ea-9d1f-121a5a85f33e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "# Remove the target column from the input feature set.\n",
    "featuresCols = data.columns\n",
    "featuresCols.remove('total_amount')\n",
    "featuresCols.remove('dropoff_at')\n",
    "featuresCols.remove('pickup_at')\n",
    "featuresCols.remove('store_and_fwd_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dea2f719-a84f-4fcf-842e-1e5550065811",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vectorAssembler combines all feature columns into a single feature vector column, \"rawFeatures\".\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\", handleInvalid=\"skip\")\n",
    "\n",
    "# vectorIndexer identifies categorical features and indexes them, and creates a new column \"features\". \n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=100, handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "212ea396-5bee-42de-9130-6c0abb78449a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(labelCol=\"total_amount\", featuresCol=\"features\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73bf5578-1f64-425f-b5b9-d9cc34971d26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a899789a-0c18-44af-b2ee-7cdf519a4ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96c396d3-3a6a-4ee5-9eb4-1bf3b280070f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3980f5f1-45e8-42ec-a8b3-a5d0c67f7830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+\n",
      "|         prediction|total_amount|            features|\n",
      "+-------------------+------------+--------------------+\n",
      "|-0.7248450083799632|      -127.8|[3.367254613393E1...|\n",
      "| 11.817054733570682|      -120.3|[3.367255226508E1...|\n",
      "|-0.7248450083799632|     -104.13|[3.367254605337E1...|\n",
      "|-0.7248450083799632|      -100.8|[3.058017471584E1...|\n",
      "|-0.7248450083799632|      -100.8|[3.367256683533E1...|\n",
      "+-------------------+------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"total_amount\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16091c7-3134-4771-9b85-c80c897ae6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a111bc-5b93-4c88-a27c-339c5cf68df0",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "\n",
    "The goal is to predict the `total_amount` (typically called the `fare`) of each ride.  To simplify the pre-processing, we may want to drop certain features like `pickup_at` and `dropoff_at` since taxi fares do not depend on the time of day, typically (unlike ride-share fares like Uber and Lyft).\n",
    "\n",
    "We may also want to drop unused fields like `store_and_fwd_flag` which is an edge case where the taxi-meter was disconnected during the trip.  This should not impact the fare.\n",
    "\n",
    "TODO:  Describe why we should drop the `rate_code_id` - or otherwise explain how it could be used.\n",
    "\n",
    "Lastly, the `payment_type` are not useful for this predictive model as the fare should not depend on how the user is paying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fb34c52-6799-4e35-94c6-62b640dfcf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Data type string of column vendor_id is not supported.\n",
    "# # Data type timestamp of column pickup_at is not supported.\n",
    "# # Data type timestamp of column dropoff_at is not supported.\n",
    "# # Data type string of column rate_code_id is not supported.\n",
    "# # Data type string of column store_and_fwd_flag is not supported.\n",
    "# # Data type string of column payment_type is not supported.\n",
    "\n",
    "# df = df.drop(\"vendor_id\").drop(\"pickup_at\").drop(\"dropoff_at\").drop(\"rate_code_id\").drop(\"store_and_fwd_flag\").drop(\"payment_type\")\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a76647a-552b-4d8c-91c0-4e80358d54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938e752a-10dc-4637-81f2-a5f50b606089",
   "metadata": {},
   "source": [
    "#### Split data into training and test sets\n",
    "\n",
    "Randomly split data into training and test sets. By doing this, you can train and tune the model using only the training subset, and then evaluate the model's performance on the test set to get a sense of how the model will perform on new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df035902-5aeb-44e9-8615-91eb057a693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the dataset randomly into 70% for training and 30% for testing. Passing a seed for deterministic behavior\n",
    "# train, test = df.randomSplit([0.7, 0.3], seed = 0)\n",
    "# print(\"There are %d training examples and %d test examples.\" % (train.count(), test.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5e5d3e-1b3e-433b-96ae-0d049b9eb5ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize the data\n",
    "You can plot the data to explore it visually. The following plot shows the number of bicycle rentals during each hour of the day.  As you might expect, rentals are low during the night, and peak at commute hours.  \n",
    "\n",
    "To create plots, call `display()` on a DataFrame in Databricks and click the plot icon below the table.\n",
    "\n",
    "To create the plot shown, run the command in the following cell. The results appear in a table. From the drop-down menu below the table, select \"Line\". Click **Plot Options...**. In the dialog, drag `hr` to the **Keys** field, and drag `cnt` to the **Values** field. Also in the **Keys** field, click the \"x\" next to `<id>` to remove it. In the **Aggregation** drop down, select \"AVG\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "990a103e-55af-49c4-8d1b-e52eddba5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.select(\"passenger_count\", \"total_amount\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e01a9-387b-4b9c-a321-c0e0351e689b",
   "metadata": {},
   "source": [
    "## Train the machine learning pipeline\n",
    "\n",
    "Now that you have reviewed the data and prepared it as a DataFrame with numeric values, you're ready to train a model to predict future bike sharing rentals. \n",
    "\n",
    "Most MLlib algorithms require a single input column containing a vector of features and a single target column. The DataFrame currently has one column for each feature. MLlib provides functions to help you prepare the dataset in the required format.\n",
    "\n",
    "MLlib pipelines combine multiple steps into a single workflow, making it easier to iterate as you develop the model. \n",
    "\n",
    "In this example, you create a pipeline using the following functions:\n",
    "* `VectorAssembler`: Assembles the feature columns into a feature vector.\n",
    "* `VectorIndexer`: Identifies columns that should be treated as categorical. This is done heuristically, identifying any column with a small number of distinct values as categorical.\n",
    "* `SparkXGBRegressor`: Uses the [SparkXGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.spark.SparkXGBRegressor) estimator to learn how to predict the fare from the feature vectors.\n",
    "* `CrossValidator`: The XGBoost regression algorithm has several hyperparameters. This notebook illustrates how to use [hyperparameter tuning in Spark](https://spark.apache.org/docs/latest/ml-tuning.html). This capability automatically tests a grid of hyperparameters and chooses the best resulting model.\n",
    "\n",
    "For more information:  \n",
    "[VectorAssembler](https://spark.apache.org/docs/latest/ml-features.html#vectorassembler)  \n",
    "[VectorIndexer](https://spark.apache.org/docs/latest/ml-features.html#vectorindexer)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1338e9-3b0a-4d12-96fe-a52f21df456d",
   "metadata": {
    "tags": []
   },
   "source": [
    "The first step is to create the VectorAssembler and VectorIndexer steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305afd09-8492-49fc-aff1-5cdf8ab14e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "# # Remove the target column from the input feature set.\n",
    "# featuresCols = df.columns\n",
    "# featuresCols.remove('total_amount')\n",
    "\n",
    "# # vectorAssembler combines all feature columns into a single feature vector column, \"rawFeatures\".\n",
    "# vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\", handleInvalid=\"skip\")\n",
    "\n",
    "# # vectorIndexer identifies categorical features and indexes them, and creates a new column \"features\". \n",
    "# vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=100, handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2756047-dd80-4d91-8b98-0c0c945f971e",
   "metadata": {},
   "source": [
    "Next, define the model. To use distributed training, set `num_workers` to the number of spark tasks you want to concurrently run during training xgboost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eba51df2-af90-4377-a461-52f8b80513c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost==1.6.2 in /opt/conda/envs/sm_glue_is/lib/python3.8/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/sm_glue_is/lib/python3.8/site-packages (from xgboost==1.6.2) (1.22.4)\n",
      "Requirement already satisfied: scipy in /opt/conda/envs/sm_glue_is/lib/python3.8/site-packages (from xgboost==1.6.2) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install xgboost==1.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402684c3-2e60-450c-b3b1-3731c94bd936",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::079002598131:role/service-role/AmazonSageMaker-ExecutionRole-20220804T150518\n",
      "Trying to create a Glue session for the kernel.\n",
      "Worker Type: G.1X\n",
      "Number of Workers: 10\n",
      "Session ID: 98460f0e-a17a-489e-8d14-86a479846284\n",
      "Job Type: glueetl\n",
      "Applying the following default arguments:\n",
      "--glue_kernel_version 0.37.0\n",
      "--enable-glue-datacatalog true\n",
      "--additional-python-modules #xgboost==1.7.1\n",
      "Waiting for session 98460f0e-a17a-489e-8d14-86a479846284 to get into ready status...\n"
     ]
    }
   ],
   "source": [
    "# import xgboost\n",
    "# print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e68298ff-4f9f-445f-9270-dfcbb5fb4e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "# # The next step is to define the model training stage of the pipeline. \n",
    "# # The following command defines a XgboostRegressor model that takes an input column \"features\" by default and learns to predict the labels in the \"cnt\" column.\n",
    "# # Set `num_workers` to the number of spark tasks you want to concurrently run during training xgboost model.\n",
    "# xgb_regressor = SparkXGBRegressor(label_col=\"total_amount\", missing=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28453fca-46f7-4440-92bc-e951a32aeda6",
   "metadata": {},
   "source": [
    "The third step is to wrap the model you just defined in a `CrossValidator` stage. `CrossValidator` calls the XgboostRegressor estimator with different hyperparameter settings. It trains multiple models and selects the best one, based on minimizing a specified metric. In this example, the metric is [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb84b8ff-bf93-4aa9-bcdb-ae3157ff0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "# from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# # Define a grid of hyperparameters to test:\n",
    "# #  - maxDepth: maximum depth of each decision tree \n",
    "# #  - maxIter: iterations, or the total number of trees \n",
    "# paramGrid = ParamGridBuilder()\\\n",
    "#   .addGrid(xgb_regressor.max_depth, [2, 5])\\\n",
    "#   .addGrid(xgb_regressor.n_estimators, [10, 100])\\\n",
    "#   .build()\n",
    "\n",
    "# # Define an evaluation metric.  The CrossValidator compares the true labels with predicted values for each combination of parameters, and calculates this value to determine the best model.\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\",\n",
    "#                                 labelCol=xgb_regressor.getLabelCol(),\n",
    "#                                 predictionCol=xgb_regressor.getPredictionCol())\n",
    "\n",
    "# # Declare the CrossValidator, which performs the model tuning.\n",
    "# cv = CrossValidator(estimator=xgb_regressor, evaluator=evaluator, estimatorParamMaps=paramGrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5b7e91-13b6-44b0-9d1e-f61352d03dc0",
   "metadata": {},
   "source": [
    "Create the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efd438a1-a865-4d45-867e-3aa57e69c785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "# pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c64fc0c2-8dc1-41b9-b92d-ebe32dc10897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.ml import Pipeline\n",
    "# pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d6dad4-fbe4-4857-906d-f8df990f95af",
   "metadata": {},
   "source": [
    "Train the pipeline.\n",
    "\n",
    "Now that you have set up the workflow, you can train the pipeline with a single call.  \n",
    "When you call `fit()`, the pipeline runs feature processing, model tuning, and training and returns a fitted pipeline with the best model it found.\n",
    "This step takes several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e302f1b-fd31-4ef4-b1a8-430fcd497f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e71c1f-04f4-424d-99d2-25c8d56a4a06",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make predictions and evaluate results\n",
    "\n",
    "The final step is to use the fitted model to make predictions on the test dataset and evaluate the model's performance. The model's performance on the test dataset provides an approximation of how it is likely to perform on new data.\n",
    "\n",
    "Computing evaluation metrics is important for understanding the quality of predictions, as well as for comparing models and tuning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1b74c4d-a463-408d-8672-0dce534e2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad017a9e-14a1-4384-b603-58f6de65f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7710abfa-dd12-435d-973b-3858554d48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.select(\"total_amount\", \"prediction\", *featuresCols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b867d2-0214-4cc4-ac4a-36b74673a60d",
   "metadata": {},
   "source": [
    "The `transform()` method of the pipeline model applies the full pipeline to the input dataset. The pipeline applies the feature processing steps to the dataset and then uses the fitted XGBoost Regressor model to make predictions. The pipeline returns a DataFrame with a new column `predictions`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c63c20-40c4-420e-a0ff-c7722b48a0a2",
   "metadata": {},
   "source": [
    "A common way to evaluate the performance of a regression model is the calculate the [root mean squared error (RMSE)](https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#regression-model-evaluation). The value is not very informative on its own, but you can use it to compare different models. `CrossValidator` determines the best model by selecting the one that minimizes RMSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2336942-8379-426d-98d8-a521cce9813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse = evaluator.evaluate(predictions)\n",
    "# print(\"RMSE on our test set: %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6ea8e70-4246-4213-989c-39be60ceb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse = evaluator.evaluate(predictions)\n",
    "# print(\"RMSE on our test set: %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173ff26-842c-45e1-9137-2597067341d9",
   "metadata": {},
   "source": [
    "You can also plot the results, as you did the original dataset. In this case, the hourly count of rentals shows a similar shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c10959c-0e27-4bbb-b91e-a28f2b36aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.select(\"passenger_count\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57286f1f-94d4-4b59-84e3-0fabbfdb9ede",
   "metadata": {},
   "source": [
    "## Save and reload the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7201b49-2980-481c-aae6-db648fd3e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%sh is a cell magic, but the cell body is empty.\n",
      "session_id=efa0d3dc-d13c-48ee-8f9c-7ee94a131c8f has reached TIMEOUT status. \n",
      "Please re-run the same cell to restart the session. You may also need to re-run previous cells if trying to use pre-defined variables.\n"
     ]
    }
   ],
   "source": [
    "#%%sh\n",
    "\n",
    "#rm -rf /dbfs/tmp/xgboost/pipeline_001\n",
    "#rm -rf /dbfs/tmp/xgboost/pipelineModel_001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "932095f4-be51-44ac-abcb-b40b3f394814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the pipeline that created the model\n",
    "# pipeline.save('/tmp/xgboost/pipeline_001')\n",
    "\n",
    "# # Save the model itself\n",
    "# pipelineModel.save('/tmp/xgboost/pipelineModel_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47ccea03-5c02-43da-94f9-e7041cde5454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the pipeline\n",
    "# loaded_pipeline = Pipeline.load('/tmp/xgboost/pipeline_001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afa0bd1b-39de-4638-86fb-fdee21c84bc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load and use the model\n",
    "# from pyspark.ml import PipelineModel\n",
    "\n",
    "# loaded_pipelineModel = PipelineModel.load('/tmp/xgboost/pipelineModel_001')\n",
    "\n",
    "# # To represent new data, use the first 3 rows of the test dataset\n",
    "# new_data = test.limit(3)\n",
    "\n",
    "# # Make predictions with the loaded model\n",
    "# new_preds = loaded_pipelineModel.transform(new_data)\n",
    "# display(new_preds.select(\"total_amount\", \"prediction\", *featuresCols))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Glue Python [PySpark and Ray] (SparkAnalytics 1.0)",
   "language": "python",
   "name": "conda-env-sm_glue_is-glue_pyspark__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-sparkanalytics-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
