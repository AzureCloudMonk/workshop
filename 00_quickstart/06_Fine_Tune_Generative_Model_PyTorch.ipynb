{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-tune the model with the Amazon Customer Reviews Dataset and a set of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svmem(total=33242578944, available=31007662080, percent=6.7, used=1821356032, free=13780353024, active=1140105216, inactive=16285843456, buffers=2768896, cached=17638100992, shared=847872, slab=1533464576)\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "notebook_memory = psutil.virtual_memory()\n",
    "print(notebook_memory)\n",
    "\n",
    "if notebook_memory.total < 32 * 1000 * 1000 * 1000:\n",
    "    print('*******************************************')    \n",
    "    print('YOU ARE NOT USING THE CORRECT INSTANCE TYPE')\n",
    "    print('PLEASE CHANGE INSTANCE TYPE TO  m5.2xlarge ')\n",
    "    print('*******************************************')\n",
    "else:\n",
    "    correct_instance_type=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-WGBCO343l-q",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_checkpoint = \"facebook/opt-350m\"\n",
    "model_checkpoint = \"bigscience/bloomz-560m\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create prompts for few-shot, one-shot, zero-shot inference on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe (145427, 15)\n",
      "Generate review headline based on review body\n",
      "Generate review based on rating and category\n",
      "Given the review headline return a categorical rating\n",
      "Generate review headline based on rating\n",
      "Given the review body return a categorical rating\n",
      "1 ||| 2 ||| 3 ||| 4 ||| 5\n",
      "{'answer_choices': '1 ||| 2 ||| 3 ||| 4 ||| 5', 'id': 'e6a1bbde-715d-4dad-9178-e2bcfaf5c646', 'jinja': \"Given the following review:\\n{{review_body}}\\npredict the associated rating from the following choices (1 being lowest and 5 being highest)\\n- {{ answer_choices | join('\\\\n- ') }} \\n|||\\n{{answer_choices[star_rating-1]}}\", 'metadata': <promptsource.templates.Template.Metadata object at 0x7f0a69f8d350>, 'name': 'Given the review body return a categorical rating', 'reference': 'Given the review body, return a categorical rating. '}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bf2faeb18af4a0d8a7e28a44511fca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the following review:\n",
      "I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "file = './data-tsv/amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz'\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(file, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, compression=\"gzip\")\n",
    "\n",
    "df.isna().values.any()\n",
    "df = df.dropna()\n",
    "df = df.reset_index(drop=True)    \n",
    "\n",
    "print(\"Shape of dataframe {}\".format(df.shape))\n",
    "\n",
    "# Convert Pandas dataframes into Datasets\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "\n",
    "# Create Dataset objects (Arrow PyTables) from Pandas dataframes\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Apply prompt    \n",
    "from promptsource.templates import DatasetTemplates\n",
    "prompt_templates = DatasetTemplates('amazon_us_reviews/Wireless_v1_00') \n",
    "\n",
    "for template in prompt_templates.templates.values():\n",
    "    print(template.get_name())\n",
    "\n",
    "prompt = prompt_templates[\"Given the review body return a categorical rating\"]\n",
    "print(prompt.answer_choices)    \n",
    "print(prompt.__dict__)\n",
    "\n",
    "dataset = dataset.select([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).map(lambda row : {'prompt': prompt.apply(row)[0], 'label': prompt.apply(row)[1]})\n",
    "prompt = dataset[0]['prompt']\n",
    "label = dataset[0]['label']\n",
    "print(prompt)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt0 = dataset[0]\n",
    "prompt1 = dataset[1]\n",
    "prompt2 = dataset[2]\n",
    "prompt3 = dataset[3]\n",
    "\n",
    "few_shot_prompt = 'PROMPT: ' + prompt0['prompt'] + '\\nRESPONSE: ' + prompt0['label'] + '\\n\\nPROMPT: ' + prompt1['prompt'] + '\\nRESPONSE: ' + prompt1['label'] + '\\n\\nPROMPT: ' + prompt2['prompt'] + '\\nRESPONSE: ' + prompt2['label'] + '\\n\\nPROMPT: ' + prompt3['prompt'] + '\\nRESPONSE:'\n",
    "one_shot_prompt = 'PROMPT: ' + prompt0['prompt'] + '\\nRESPONSE: ' + prompt0['label'] + '\\n\\nPROMPT: ' + prompt1['prompt'] + '\\nRESPONSE:'\n",
    "zero_shot_prompt = 'PROMPT: ' + prompt0['prompt'] + '\\nRESPONSE:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform few-shot, one-shot, zero-shot inference BEFORE fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5io6fY_d3l-u"
   },
   "source": [
    "To tokenize all our texts with the same vocabulary that was used when training the model, we have to download a pretrained tokenizer. This is all done by the `AutoTokenizer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iAYlS40Z3l-v",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpOiBrJ13l-y"
   },
   "source": [
    "We can now call the tokenizer on all our texts. This is very simple, using the [`map`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map) method from the Datasets library. First we define a function that call the tokenizer on our texts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEmeQ7Xm3l_H"
   },
   "source": [
    "Now that the data has been loaded, we're ready to instantiate our `Trainer`. We will retrieve our pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "sPqQA3TT3l_I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Given the following review:\n",
      "I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 2\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "Awesome\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 5\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "If you are prepping for the end of the world this is one of those things that you should have installed on your-end-of-the-world-proof PC.  Hail to the great Yuri!\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 5\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "Perfect\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 5\n",
      "EXPECTED RESPONSE: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_new_tokens=1,\n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "\n",
    "print('EXPECTED RESPONSE: {}'.format(prompt3['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Given the following review:\n",
      "I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 2\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "Awesome\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 2\n",
      "EXPECTED RESPONSE: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_new_tokens=1,\n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "\n",
    "print('EXPECTED RESPONSE: {}'.format(prompt1['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Given the following review:\n",
      "I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 2\n",
      "EXPECTED RESPONSE: 2\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(zero_shot_prompt, return_tensors='pt')\n",
    "\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_new_tokens=1,\n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "\n",
    "print('EXPECTED RESPONSE: {}'.format(prompt0['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model with the Amazon Customer Reviews Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-0b7843339d711258\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/parquet/default-0b7843339d711258/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36, 3)\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "lm_dataset_train = Dataset.from_parquet('./data/train/*.parquet')\n",
    "print(lm_dataset_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "YbSwEhQ63l_L",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-amazon-customer-reviews\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01, \n",
    "    max_steps=10,\n",
    "    num_train_epochs=1.0,\n",
    "    no_cuda=not torch.cuda.is_available()    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZRbT9ui3l_N"
   },
   "source": [
    "We pass along all of those to the `Trainer` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "OEuqwIra3l_N",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset_train\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NyZvu_MF3l_P",
    "outputId": "b69d0931-7f1f-4f2d-fdb8-09d37c7418bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 36\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "  Number of trainable parameters = 559214592\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=9.286512756347657, metrics={'train_runtime': 117.0914, 'train_samples_per_second': 0.683, 'train_steps_per_second': 0.085, 'total_flos': 16716725747712.0, 'train_loss': 9.286512756347657, 'epoch': 2.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "train_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in ./tmp_models/bigscience/bloomz-560m/config.json\n",
      "Configuration saved in ./tmp_models/bigscience/bloomz-560m/generation_config.json\n",
      "Model weights saved in ./tmp_models/bigscience/bloomz-560m/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "model_path = './tmp_models/{}/'.format(model_checkpoint)\n",
    "\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform few-shot, one-shot, zero-shot inference AFTER fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/64a6f1765615d2c38a7fe4474b8bff5a6c0e14e6/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/64a6f1765615d2c38a7fe4474b8bff5a6c0e14e6/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bigscience--bloomz-560m/snapshots/64a6f1765615d2c38a7fe4474b8bff5a6c0e14e6/tokenizer_config.json\n",
      "loading configuration file ./tmp_models/bigscience/bloomz-560m/config.json\n",
      "Model config BloomConfig {\n",
      "  \"_name_or_path\": \"./tmp_models/bigscience/bloomz-560m/\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "loading weights file ./tmp_models/bigscience/bloomz-560m/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing BloomForCausalLM.\n",
      "\n",
      "All the weights of BloomForCausalLM were initialized from the model checkpoint at ./tmp_models/bigscience/bloomz-560m/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BloomForCausalLM for predictions without further training.\n",
      "loading configuration file ./tmp_models/bigscience/bloomz-560m/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model also supports many advanced parameters while performing inference including the following:\n",
    "\n",
    "**max_length**: Model generates text until the output length (which includes the input context length) reaches max_length. If specified, it must be a positive integer.\n",
    "\n",
    "**num_return_sequences**: Number of output sequences returned. If specified, it must be a positive integer.\n",
    "\n",
    "**num_beams**: Number of beams used in the greedy search. If specified, it must be integer greater than or equal to num_return_sequences.\n",
    "\n",
    "**no_repeat_ngram_size**: Model ensures that a sequence of words of no_repeat_ngram_size is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "\n",
    "**temperature**: Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If temperature -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "\n",
    "**early_stopping**: If True, text generation is finished when all beam hypotheses reach the end of stence token. If specified, it must be boolean.\n",
    "\n",
    "**do_sample**: If True, sample the next word as per the likelyhood. If specified, it must be boolean.\n",
    "\n",
    "**top_k**: In each step of text generation, sample from only the top_k most likely words. If specified, it must be a positive integer.\n",
    "\n",
    "**top_p**: In each step of text generation, sample from the smallest possible set of words with cumulative probability top_p. If specified, it must be a float between 0 and 1.\n",
    "\n",
    "**seed**: Fix the randomized state for reproducibility. If specified, it must be an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Given the following review:\n",
      "I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 2\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "Awesome\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 5\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "If you are prepping for the end of the world this is one of those things that you should have installed on your-end-of-the-world-proof PC.  Hail to the great Yuri!\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 5\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "Perfect\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 5\n",
      "EXPECTED RESPONSE: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(few_shot_prompt, return_tensors='pt')\n",
    "\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_new_tokens=1,\n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "\n",
    "print('EXPECTED RESPONSE: {}'.format(prompt3['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Given the following review:\n",
      "I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 2\n",
      "\n",
      "PROMPT: Given the following review:\n",
      "Awesome\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 2\n",
      "EXPECTED RESPONSE: 5\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(one_shot_prompt, return_tensors='pt')\n",
    "\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_new_tokens=1,\n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "\n",
    "print('EXPECTED RESPONSE: {}'.format(prompt1['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROMPT: Given the following review:\n",
      "I keep buying madden every year hoping they get back to football. This years version is a little better than last years -- but that's not saying much.The game looks great. The only thing wrong with the animation, is the way the players are always tripping on each other.<br /><br />The gameplay is still slowed down by the bloated pre-play controls. What used to take two buttons is now a giant PITA to get done before an opponent snaps the ball or the play clock runs out.<br /><br />The turbo button is back, but the player movement is still slow and awkward. If you liked last years version, I'm guessing you'll like this too. I haven't had a chance to play anything other than training and a few online games, so I'm crossing my fingers and hoping the rest is better.<br /><br />The one thing I can recommend is NOT TO BUY THE MADDEN BUNDLE. The game comes as a download. So if you hate it, there's no trading it in at Gamestop.\n",
      "predict the associated rating from the following choices (1 being lowest and 5 being highest)\n",
      "- 1\n",
      "- 2\n",
      "- 3\n",
      "- 4\n",
      "- 5\n",
      "RESPONSE: 3\n",
      "EXPECTED RESPONSE: 2\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(zero_shot_prompt, return_tensors='pt')\n",
    "\n",
    "print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "                       max_new_tokens=1,\n",
    "                       do_sample=True, \n",
    "                       top_k=50, \n",
    "                       top_p=0.9\n",
    "                      )[0]))\n",
    "\n",
    "print('EXPECTED RESPONSE: {}'.format(prompt0['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./tmp_models/bigscience/bloomz-560m/config.json\n",
      "Model config BloomConfig {\n",
      "  \"_name_or_path\": \"./tmp_models/bigscience/bloomz-560m/\",\n",
      "  \"apply_residual_connection_post_layernorm\": false,\n",
      "  \"architectures\": [\n",
      "    \"BloomForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_softmax_in_fp32\": true,\n",
      "  \"bias_dropout_fusion\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"masked_softmax_fusion\": true,\n",
      "  \"model_type\": \"bloom\",\n",
      "  \"n_head\": 16,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"offset_alibi\": 100,\n",
      "  \"pad_token_id\": 3,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"seq_length\": 2048,\n",
      "  \"skip_bias_add\": true,\n",
      "  \"skip_bias_add_qkv\": false,\n",
      "  \"slow_but_exact\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"unk_token_id\": 0,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250880\n",
      "}\n",
      "\n",
      "Loading the model in mixed int8 - forcing the weights to be casted in float16\n",
      "loading weights file ./tmp_models/bigscience/bloomz-560m/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2236858368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Instantiating BloomForCausalLM model under default dtype torch.float16.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'init_empty_weights' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-c40aba997d63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_memory_footprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mquantized_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_empty_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquantized_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_memory_footprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             return model_class.from_pretrained(\n\u001b[0;32m--> 465\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             )\n\u001b[1;32m    467\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m             \u001b[0minit_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict_or_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeepspeed_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minit_contexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mload_in_8bit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m             \u001b[0minit_contexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_empty_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_contexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'init_empty_weights' is not defined"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(model.get_memory_footprint())\n",
    "\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=True, device_map='auto')\n",
    "\n",
    "print(quantized_model.get_memory_footprint())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRAS (work-in-progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create Dataset objects (Arrow PyTables) from Pandas dataframes\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# # Apply prompt    \n",
    "# from promptsource.templates import DatasetTemplates\n",
    "# prompt_templates = DatasetTemplates('amazon_us_reviews/Wireless_v1_00') \n",
    "\n",
    "# for template in prompt_templates.templates.values():\n",
    "#     print(template.get_name())\n",
    "\n",
    "# prompt = prompt_templates[\"Given the review body return a categorical rating\"]\n",
    "# print(prompt.answer_choices)    \n",
    "# print(prompt.__dict__)\n",
    "\n",
    "# dataset = dataset.select([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).map(lambda row : {'prompt': prompt.apply(row)[0], 'label': prompt.apply(row)[1]})\n",
    "# prompt = dataset[0]['prompt']\n",
    "# label = dataset[0]['label']\n",
    "# print(prompt)\n",
    "# print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt0 = dataset[0]\n",
    "# prompt1 = dataset[1]\n",
    "# prompt2 = dataset[2]\n",
    "# prompt3 = dataset[3]\n",
    "\n",
    "# #few_shot_prompt = prompt0['prompt'] + '\\nFor the previous review, is the sentiment positive, neutral, or negative?' + prompt0['label'] + '\\n\\nPROMPT: ' + prompt1['prompt'] + '\\nRESPONSE: ' + prompt1['label'] + '\\n\\nPROMPT: ' + prompt2['prompt'] + '\\nRESPONSE: ' + prompt2['label'] + '\\n\\nPROMPT: ' + prompt3['prompt'] + '\\nRESPONSE:'\n",
    "# #one_shot_prompt = 'PROMPT: ' + prompt0['prompt'] + '\\nRESPONSE: ' + prompt0['label'] + '\\n\\nPROMPT: ' + prompt1['prompt'] + '\\nRESPONSE:'\n",
    "# zero_shot_prompt = prompt0['prompt'] + '\\nFor the previous review, is the sentiment positive, neutral, or negative?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# inputs = tokenizer(zero_shot_prompt, return_tensors='pt')\n",
    "\n",
    "# print(tokenizer.decode(model.generate(inputs[\"input_ids\"], \n",
    "#                        max_new_tokens=1,\n",
    "#                        do_sample=True, \n",
    "#                        top_k=50, \n",
    "#                        top_p=0.9\n",
    "#                       )[0]))\n",
    "\n",
    "# print('EXPECTED RESPONSE: {}'.format(prompt0['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "\n",
    "# <p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "# <button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "# <script>\n",
    "# try {\n",
    "#     els = document.getElementsByClassName(\"sm-command-button\");\n",
    "#     els[0].click();\n",
    "# }\n",
    "# catch(err) {\n",
    "#     // NoOp\n",
    "# }    \n",
    "# </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "colab": {
   "name": "Fine-tune a language model",
   "provenance": []
  },
  "instance_type": "ml.m5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
