{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train reward model with human feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import json\n",
    "import uuid\n",
    "import time\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Amazon Python SDK clients\n",
    "sagemaker = boto3.client(\"sagemaker\", region)\n",
    "a2i = boto3.client(\"sagemaker-a2i-runtime\")\n",
    "s3 = boto3.client(\"s3\", region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the `human_loop_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store -r human_loop_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     human_loop_name\n",
    "# except NameError:\n",
    "#     print(\"*** PLEASE RUN PREVIOUS NOTEBOOK BEFORE CONTINUING ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(human_loop_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the Human Loops are Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# completed_human_loops = []\n",
    "# for human_loop_name in human_loops_started:\n",
    "#     resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "#     print(f\"HumanLoop Name: {human_loop_name}\")\n",
    "#     print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "#     print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "#     print(\"\")\n",
    "#     while resp[\"HumanLoopStatus\"] != \"Completed\":\n",
    "#         print(f\"Waiting for HumanLoop to complete.\")\n",
    "#         time.sleep(10)\n",
    "#         resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "#     if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "#         completed_human_loops.append(resp)\n",
    "#         print(f\"Completed!\")\n",
    "#         print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View Human Labels  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the work is complete, Amazon A2I stores the results in the specified S3 bucket and sends a Cloudwatch Event.  Let's check the S3 contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# import pprint\n",
    "\n",
    "# pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# human_feedback_items = []\n",
    "# human\n",
    "# for resp in completed_human_loops:\n",
    "#     human_feedback_s3_uri = \"s3://\" + bucket + \"/\", resp[\"HumanLoopOutput\"][\"OutputS3Uri\"]\n",
    "#     split_string = re.split(human_feedback_s3_uri)\n",
    "#     output_bucket_key = split_string[1]\n",
    "\n",
    "#     response = s3.get_object(Bucket=bucket, Key=output_bucket_key)\n",
    "#     content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "#     json_output = json.loads(content)\n",
    "#     print(json_output)\n",
    "\n",
    "#     input_content = json_output[\"inputContent\"]\n",
    "#     human_answer = json_output[\"humanAnswers\"][0][\"answerContent\"]\n",
    "#     human_feedback_item = {\"input_content\": input_content, \"human_answer\": human_answer, s3_uri: \"s3_uri\"}\n",
    "#     human_feedback_items.append(human_feedback_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for Re-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_human_feedback_items = pd.DataFrame(human_feedback_items)\n",
    "# df_human_feedback_items.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a reward model with human preference, instruction, and alignment data\n",
    "This is typically a language model initialized from the supervised-fine-tuned (SFT) model (trained in a previous notebook), but with an additional binary-classification layer placed on top.  This reward model is used to train the reinforcement-learning model in the next step.  The reinforcement-learning model is what is deployed into production to serve applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_checkpoint\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the notebooks in the previous section before you continue.\")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r supervised_fine_tuned_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    supervised_fine_tuned_model_path\n",
    "except NameError:\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"[ERROR] Please run the notebooks in the previous section before you continue.\")\n",
    "    print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(supervised_fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_list_input_files(path):\n",
    "    input_files = glob.glob('{}/*.parquet'.format(path))\n",
    "    print(input_files)\n",
    "    return input_files\n",
    "\n",
    "def save_transformer_model(model, model_dir):\n",
    "    path = os.path.join(model_dir, 'transformer')\n",
    "    os.makedirs(path, exist_ok=True)                              \n",
    "    print('Saving Transformer model to {}'.format(path))\n",
    "    model.save_pretrained(path)\n",
    "\n",
    "def save_pytorch_model(model, model_checkpoint, model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True) \n",
    "    print('Saving PyTorch model to {}'.format(model_dir))\n",
    "    save_path = os.path.join(model_dir, model_checkpoint.replace('/', '-'))\n",
    "    torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PyTorch Dataset and DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PyTorch dataset retrieves the dataset’s features and labels one sample at a time\n",
    "# Create a custom Dataset class for the reviews\n",
    "class ReviewDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_ids_list, label_id_list):\n",
    "        self.input_ids_list = input_ids_list\n",
    "        self.label_id_list = label_id_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # convert list of token_ids into an array of PyTorch LongTensors\n",
    "        input_ids = json.loads(self.input_ids_list[item]) \n",
    "        label_id = self.label_id_list[item]\n",
    "\n",
    "        input_ids_tensor = torch.LongTensor(input_ids)\n",
    "        label_id_tensor = torch.tensor(label_id, dtype=torch.long)\n",
    "\n",
    "        return input_ids_tensor, label_id_tensor\n",
    "\n",
    "    \n",
    "# PyTorch DataLoader helps to to organise the input training data in “minibatches” and reshuffle the data at every epoch\n",
    "# It takes Dataset as an input\n",
    "def create_data_loader(path, batch_size): \n",
    "    print(\"Get data loader\")\n",
    "\n",
    "    df = pd.DataFrame(columns=['input_ids', 'label_id'])\n",
    "    \n",
    "    input_files = create_list_input_files(path)\n",
    "\n",
    "    for file in input_files:\n",
    "        # df_temp = pd.read_csv(file, \n",
    "        #                       sep='\\t', \n",
    "        #                       usecols=['input_ids', 'label_id'])\n",
    "        df_temp = pd.read_parquet(file)\n",
    "        df = df.append(df_temp)\n",
    "        print('adding df_temp: {}'.format(df_temp))\n",
    "        \n",
    "    ds = ReviewDataset(\n",
    "        input_ids_list=df.input_ids.to_numpy(),\n",
    "        label_id_list=df.label_id.to_numpy(),\n",
    "    )\n",
    "    \n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    ), df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO:  Change this to binary classification\n",
    "#        where 1 is assigned to the human-selected (presumably-correct) label\n",
    "#        and 0 is assigned to all of other labels\n",
    "\n",
    "def get_model_config():\n",
    "    classes = [1, 2, 3, 4, 5]\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        supervised_fine_tuned_model_path,        \n",
    "        num_labels=len(classes),\n",
    "        id2label={\n",
    "            0: 1, \n",
    "            1: 2, \n",
    "            2: 3, \n",
    "            3: 4, \n",
    "            4: 5            \n",
    "        },\n",
    "        label2id={\n",
    "            1: 0,\n",
    "            2: 1,\n",
    "            3: 2,\n",
    "            4: 3,\n",
    "            5: 4\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    config.output_attentions=True\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model,\n",
    "                train_data_loader,\n",
    "                df_train,\n",
    "                val_data_loader, \n",
    "                df_val,\n",
    "                args):\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()    \n",
    "    optimizer = optim.Adam(params=model.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    if args.freeze_base_layers:\n",
    "        print('Freezing base layers...')\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier' not in name:  # classifier layer\n",
    "                param.requires_grad = False\n",
    "        print('Set classifier layers to `param.requires_grad=False`.')        \n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        print('EPOCH -- {}'.format(epoch))\n",
    "\n",
    "        for i, (sent, label) in enumerate(train_data_loader):\n",
    "            print('i: ' + i)\n",
    "            print('sent: ' + sent)\n",
    "            print('label: ' + label)            \n",
    "            if i < args.train_steps_per_epoch:\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                sent = sent.squeeze(0)\n",
    "                if torch.cuda.is_available():\n",
    "                    sent = sent.cuda()\n",
    "                    label = label.cuda()\n",
    "                output = model(sent)[0]\n",
    "                _, predicted = torch.max(output, 1)\n",
    "\n",
    "                loss = loss_function(output, label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "                if args.run_validation and i % args.validation_steps_per_epoch == 0:\n",
    "                    print('RUNNING VALIDATION:')\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    model.eval()\n",
    "\n",
    "                    for sent, label in val_data_loader:\n",
    "                        sent = sent.squeeze(0)\n",
    "                        if torch.cuda.is_available():\n",
    "                            sent = sent.cuda()\n",
    "                            label = label.cuda()\n",
    "                        output = model(sent)[0]\n",
    "                        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                        total += label.size(0)\n",
    "                        correct += (predicted.cpu() ==label.cpu()).sum()\n",
    "\n",
    "                    accuracy = 100.00 * correct.numpy() / total\n",
    "                    print('[epoch/step: {0}/{1}] val_loss: {2:.2f} - val_acc: {3:.2f}%'.format(epoch, i, loss.item(), accuracy))\n",
    "            else:\n",
    "                break           \n",
    "\n",
    "    print('TRAINING COMPLETED.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "    \n",
    "# Parse args\n",
    "\n",
    "\n",
    "os.environ['SM_HOSTS'] = '{\"hosts\": [\"algo-1\"]}'\n",
    "os.environ['SM_CURRENT_HOST'] = 'algo-1'\n",
    "os.environ['SM_NUM_GPUS'] = '0'\n",
    "os.environ['SM_MODEL_DIR'] = './model/reward_model/'\n",
    "os.environ['SM_CHANNEL_TRAIN'] = './data/train'\n",
    "os.environ['SM_CHANNEL_VALIDATION'] = './data/validation'\n",
    "os.environ['SM_OUTPUT_DIR'] = './model_output/'\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# CLI args\n",
    "\n",
    "parser.add_argument('--train_batch_size', \n",
    "                    type=int, \n",
    "                    default=64)\n",
    "\n",
    "parser.add_argument('--train_steps_per_epoch',\n",
    "                    type=int,\n",
    "                    default=64)\n",
    "\n",
    "parser.add_argument('--validation_batch_size', \n",
    "                    type=int, \n",
    "                    default=64)\n",
    "\n",
    "parser.add_argument('--validation_steps_per_epoch',\n",
    "                    type=int,\n",
    "                    default=64)\n",
    "\n",
    "parser.add_argument('--epochs', \n",
    "                    type=int, \n",
    "                    default=10)\n",
    "\n",
    "parser.add_argument('--freeze_base_layers', \n",
    "                    type=eval, \n",
    "                    default=False)\n",
    "\n",
    "parser.add_argument('--learning_rate', \n",
    "                    type=float, \n",
    "                    default=0.01)\n",
    "\n",
    "parser.add_argument('--momentum', \n",
    "                    type=float, \n",
    "                    default=0.5)\n",
    "\n",
    "parser.add_argument('--seed', \n",
    "                    type=int, \n",
    "                    default=42)\n",
    "\n",
    "parser.add_argument('--log_interval', \n",
    "                    type=int, \n",
    "                    default=100)\n",
    "\n",
    "parser.add_argument('--backend', \n",
    "                    type=str, \n",
    "                    default=None)\n",
    "\n",
    "parser.add_argument('--run_validation', \n",
    "                    type=eval,\n",
    "                    default=False)\n",
    "\n",
    "parser.add_argument('--model-checkpoint', \n",
    "                    type=str,\n",
    "                    default=model_checkpoint)\n",
    "\n",
    "\n",
    "# Container environment  \n",
    "\n",
    "parser.add_argument('--hosts', \n",
    "                    type=list, \n",
    "                    default=json.loads(os.environ['SM_HOSTS']))\n",
    "\n",
    "parser.add_argument('--current_host', \n",
    "                    type=str, \n",
    "                    default=os.environ['SM_CURRENT_HOST'])\n",
    "\n",
    "parser.add_argument('--model_dir', \n",
    "                    type=str, \n",
    "                    default=os.environ['SM_MODEL_DIR'])\n",
    "\n",
    "parser.add_argument('--train_data', \n",
    "                    type=str, \n",
    "                    default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "parser.add_argument('--validation_data', \n",
    "                    type=str, \n",
    "                    default=os.environ['SM_CHANNEL_VALIDATION'])\n",
    "\n",
    "parser.add_argument('--output_dir', \n",
    "                    type=str, \n",
    "                    default=os.environ['SM_OUTPUT_DIR'])\n",
    "\n",
    "parser.add_argument('--num_gpus', \n",
    "                    type=int, \n",
    "                    default=os.environ['SM_NUM_GPUS'])\n",
    "\n",
    "# Debugger args\n",
    "\n",
    "parser.add_argument(\"--save-frequency\", \n",
    "                    type=int, \n",
    "                    default=10, \n",
    "                    help=\"frequency with which to save steps\")\n",
    "\n",
    "parser.add_argument(\"--smdebug_path\",\n",
    "                    type=str,\n",
    "                    help=\"output directory to save data in\",\n",
    "                    default=\"/opt/ml/output/tensors\",)\n",
    "\n",
    "parser.add_argument(\"--hook-type\",\n",
    "                    type=str,\n",
    "                    choices=[\"saveall\", \"module-input-output\", \"weights-bias-gradients\"],\n",
    "                    default=\"saveall\",)\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "print('Loaded arguments:')\n",
    "print(args)\n",
    "\n",
    "# Get environment variables\n",
    "\n",
    "env_var = os.environ \n",
    "print('Environment variables:')\n",
    "pprint.pprint(dict(env_var), width = 1) \n",
    "\n",
    "# Check if distributed training\n",
    "\n",
    "is_distributed = len(args.hosts) > 1 and args.backend is not None\n",
    "\n",
    "print(\"Distributed training - {}\".format(is_distributed))\n",
    "use_cuda = args.num_gpus > 0\n",
    "print(\"Number of gpus available - {}\".format(args.num_gpus))\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "# Initialize the distributed environment.\n",
    "\n",
    "if is_distributed:\n",
    "    world_size = len(args.hosts)\n",
    "    os.environ['WORLD_SIZE'] = str(world_size)\n",
    "    host_rank = args.hosts.index(args.current_host)\n",
    "    os.environ['RANK'] = str(host_rank)\n",
    "    dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\n",
    "    print('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\n",
    "        args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\n",
    "        dist.get_rank(), args.num_gpus))\n",
    "\n",
    "# Set the seed for generating random numbers\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(args.seed) \n",
    "\n",
    "# Instantiate model\n",
    "\n",
    "config = None\n",
    "model = None\n",
    "\n",
    "successful_download = False\n",
    "retries = 0\n",
    "\n",
    "while (retries < 5 and not successful_download):\n",
    "    try:\n",
    "        # Setup model\n",
    "        config = get_model_config()\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            supervised_fine_tuned_model_path,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        model.to(device)\n",
    "        successful_download = True\n",
    "        print('Sucessfully downloaded after {} retries.'.format(retries))\n",
    "\n",
    "    except:\n",
    "        retries = retries + 1\n",
    "        random_sleep = random.randint(1, 30)\n",
    "        print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\n",
    "        time.sleep(random_sleep)\n",
    "\n",
    "if not model:\n",
    "     print('Not properly initialized...')\n",
    "\n",
    "# Create data loaders\n",
    "\n",
    "train_data_loader, df_train = create_data_loader(args.train_data, args.train_batch_size)\n",
    "val_data_loader, df_val = create_data_loader(args.validation_data, args.validation_batch_size)\n",
    "\n",
    "print(\"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "    len(train_data_loader.sampler), len(train_data_loader.dataset),\n",
    "    100. * len(train_data_loader.sampler) / len(train_data_loader.dataset)\n",
    "))\n",
    "\n",
    "print(\"Processes {}/{} ({:.0f}%) of validation data\".format(\n",
    "    len(val_data_loader.sampler), len(val_data_loader.dataset),\n",
    "    100. * len(val_data_loader.sampler) / len(val_data_loader.dataset)\n",
    ")) \n",
    "\n",
    "print('model_dir: {}'.format(args.model_dir))    \n",
    "#print('model summary: {}'.format(model))\n",
    "\n",
    "callbacks = []\n",
    "initial_epoch_number = 0\n",
    "\n",
    "# Start training\n",
    "\n",
    "model = train_model(\n",
    "    model,\n",
    "    train_data_loader,\n",
    "    df_train,\n",
    "    val_data_loader, \n",
    "    df_val,\n",
    "    args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_transformer_model(model, args.model_dir)\n",
    "save_pytorch_model(model, args.model_checkpoint, args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "reward_model_path = os.path.join(args.model_dir, 'transformer'),\n",
    "\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "\n",
    "inference_pipeline = pipeline(\"text-classification\", \n",
    "                              reward_model_path,\n",
    "                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%html\n",
    "\n",
    "# <p><b>Shutting down your kernel for this notebook to release resources.</b></p>\n",
    "# <button class=\"sm-command-button\" data-commandlinker-command=\"kernelmenu:shutdown\" style=\"display:none;\">Shutdown Kernel</button>\n",
    "        \n",
    "# <script>\n",
    "# try {\n",
    "#     els = document.getElementsByClassName(\"sm-command-button\");\n",
    "#     els[0].click();\n",
    "# }\n",
    "# catch(err) {\n",
    "#     // NoOp\n",
    "# }    \n",
    "# </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
