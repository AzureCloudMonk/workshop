{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30cf1d-2fe2-463a-9998-408d808cb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --disable-pip-version-check torch==1.13.1 torchdata==0.5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43d839c-0a9c-406e-815d-da2c9a1fd1e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s above. <==_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7ca830-071e-4a1b-af0a-2304cd9841dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hugging Face Transformers\n",
    "## _==> Please ignore all WARNINGs and ERRORs from the `pip install`'s below. <==_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e685e4c8-cac6-4a5f-95fd-a6ebdf0c113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --disable-pip-version-check -q \\\n",
    "    transformers==4.26.1 \\\n",
    "    datasets==2.9.0 \\\n",
    "    accelerate==0.17.0 \\\n",
    "    bitsandbytes==0.37.0 \\\n",
    "    promptsource==0.2.3 \\\n",
    "    evaluate==0.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db45fc3a-34db-49ad-a342-4f816618faad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset summarize_from_feedback (/root/.cache/huggingface/datasets/openai___summarize_from_feedback/comparisons/0.0.0/483f970ceb55b926b0a087ef4f678ab1b089bc8174a107a452c6152e88af7ff0)\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.91s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 72/72 [17:51<00:00, 14.88s/it]\n",
      "Some weights of BloomForSequenceClassification were not initialized from the model checkpoint at bigscience/bloomz and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#0:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#1:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  17%|█▋        | 2/12 [00:00<00:00, 16.93ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:  17%|█▋        | 2/12 [00:00<00:00, 19.38ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  17%|█▋        | 2/12 [00:00<00:00, 19.90ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#3:  17%|█▋        | 2/12 [00:00<00:00, 18.68ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  17%|█▋        | 2/12 [00:00<00:00, 17.80ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  17%|█▋        | 2/12 [00:00<00:00, 19.26ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#0:  42%|████▏     | 5/12 [00:00<00:00, 24.81ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  42%|████▏     | 5/12 [00:00<00:00, 23.86ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#1:  42%|████▏     | 5/12 [00:00<00:00, 23.58ba/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  42%|████▏     | 5/12 [00:00<00:00, 24.10ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  42%|████▏     | 5/12 [00:00<00:00, 22.68ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  42%|████▏     | 5/12 [00:00<00:00, 21.58ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  42%|████▏     | 5/12 [00:00<00:00, 20.84ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:  67%|██████▋   | 8/12 [00:00<00:00, 26.39ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  67%|██████▋   | 8/12 [00:00<00:00, 26.13ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  67%|██████▋   | 8/12 [00:00<00:00, 25.40ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#3:  67%|██████▋   | 8/12 [00:00<00:00, 24.88ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  67%|██████▋   | 8/12 [00:00<00:00, 24.46ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  67%|██████▋   | 8/12 [00:00<00:00, 24.17ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  67%|██████▋   | 8/12 [00:00<00:00, 23.95ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  67%|██████▋   | 8/12 [00:00<00:00, 22.61ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0: 100%|██████████| 12/12 [00:00<00:00, 27.56ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#1:  92%|█████████▏| 11/12 [00:00<00:00, 26.44ba/s]\u001b[A\n",
      "\n",
      "\n",
      "#3:  92%|█████████▏| 11/12 [00:00<00:00, 25.99ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  92%|█████████▏| 11/12 [00:00<00:00, 25.99ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  92%|█████████▏| 11/12 [00:00<00:00, 26.12ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  92%|█████████▏| 11/12 [00:00<00:00, 25.74ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7: 100%|██████████| 12/12 [00:00<00:00, 26.64ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1: 100%|██████████| 12/12 [00:00<00:00, 26.28ba/s]\n",
      "#3: 100%|██████████| 12/12 [00:00<00:00, 25.83ba/s]\n",
      "#4: 100%|██████████| 12/12 [00:00<00:00, 24.93ba/s]\n",
      "#5: 100%|██████████| 12/12 [00:00<00:00, 25.01ba/s]\n",
      "#2: 100%|██████████| 12/12 [00:00<00:00, 25.14ba/s]\n",
      "#6: 100%|██████████| 12/12 [00:00<00:00, 24.08ba/s]\n",
      "\n",
      "#1:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\n",
      "\n",
      "#2:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#0:  18%|█▊        | 2/11 [00:00<00:00, 19.84ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  27%|██▋       | 3/11 [00:00<00:00, 22.95ba/s]\u001b[A\u001b[A\n",
      "#1:  27%|██▋       | 3/11 [00:00<00:00, 21.89ba/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  27%|██▋       | 3/11 [00:00<00:00, 22.04ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  27%|██▋       | 3/11 [00:00<00:00, 21.90ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  27%|██▋       | 3/11 [00:00<00:00, 22.58ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  27%|██▋       | 3/11 [00:00<00:00, 22.59ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  45%|████▌     | 5/11 [00:00<00:00, 24.52ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#0:  45%|████▌     | 5/11 [00:00<00:00, 23.90ba/s]\u001b[A\u001b[A\n",
      "#1:  55%|█████▍    | 6/11 [00:00<00:00, 25.30ba/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  55%|█████▍    | 6/11 [00:00<00:00, 24.79ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  55%|█████▍    | 6/11 [00:00<00:00, 24.34ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  55%|█████▍    | 6/11 [00:00<00:00, 25.03ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  55%|█████▍    | 6/11 [00:00<00:00, 24.65ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  73%|███████▎  | 8/11 [00:00<00:00, 26.44ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#0:  73%|███████▎  | 8/11 [00:00<00:00, 25.57ba/s]\u001b[A\n",
      "\n",
      "#2:  82%|████████▏ | 9/11 [00:00<00:00, 26.68ba/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  82%|████████▏ | 9/11 [00:00<00:00, 26.53ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  82%|████████▏ | 9/11 [00:00<00:00, 26.25ba/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  82%|████████▏ | 9/11 [00:00<00:00, 26.13ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#1: 100%|██████████| 11/11 [00:00<00:00, 26.70ba/s][A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#2: 100%|██████████| 11/11 [00:00<00:00, 26.47ba/s]\n",
      "#5: 100%|██████████| 11/11 [00:00<00:00, 26.55ba/s]\n",
      "#4: 100%|██████████| 11/11 [00:00<00:00, 26.77ba/s]\n",
      "#3: 100%|██████████| 11/11 [00:00<00:00, 26.34ba/s]\n",
      "#0: 100%|██████████| 11/11 [00:00<00:00, 25.85ba/s]\n",
      "#6: 100%|██████████| 11/11 [00:00<00:00, 25.23ba/s]\n",
      "#7: 100%|██████████| 11/11 [00:00<00:00, 23.42ba/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:   0%|          | 0/12 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:   0%|          | 0/12 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "#3:   0%|          | 0/12 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "#0:   0%|          | 0/12 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\n",
      "#2:   0%|          | 0/12 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[A\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:   0%|          | 0/12 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:   0%|          | 0/12 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[A\u001b[A\u001b[A\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\n",
      "\n",
      "#3:   8%|▊         | 1/12 [00:01<00:13,  1.25s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:   8%|▊         | 1/12 [00:01<00:15,  1.38s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:   8%|▊         | 1/12 [00:01<00:15,  1.40s/ba]\u001b[A\u001b[A\n",
      "#1:   8%|▊         | 1/12 [00:01<00:15,  1.42s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:   8%|▊         | 1/12 [00:01<00:15,  1.40s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:   8%|▊         | 1/12 [00:01<00:16,  1.47s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:   8%|▊         | 1/12 [00:01<00:16,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  17%|█▋        | 2/12 [00:02<00:13,  1.36s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  17%|█▋        | 2/12 [00:02<00:13,  1.40s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  17%|█▋        | 2/12 [00:02<00:13,  1.40s/ba]\u001b[A\n",
      "\n",
      "#0:  17%|█▋        | 2/12 [00:02<00:14,  1.46s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  17%|█▋        | 2/12 [00:02<00:14,  1.44s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  17%|█▋        | 2/12 [00:02<00:14,  1.44s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  17%|█▋        | 2/12 [00:02<00:14,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  25%|██▌       | 3/12 [00:04<00:12,  1.42s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  25%|██▌       | 3/12 [00:04<00:12,  1.41s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  25%|██▌       | 3/12 [00:04<00:12,  1.40s/ba]\u001b[A\u001b[A\n",
      "#0:  25%|██▌       | 3/12 [00:04<00:13,  1.45s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  25%|██▌       | 3/12 [00:04<00:13,  1.45s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  25%|██▌       | 3/12 [00:04<00:13,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  25%|██▌       | 3/12 [00:04<00:13,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  33%|███▎      | 4/12 [00:05<00:11,  1.46s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  33%|███▎      | 4/12 [00:05<00:11,  1.44s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:  33%|███▎      | 4/12 [00:05<00:11,  1.43s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  33%|███▎      | 4/12 [00:05<00:11,  1.47s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  33%|███▎      | 4/12 [00:05<00:11,  1.47s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  33%|███▎      | 4/12 [00:05<00:11,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  33%|███▎      | 4/12 [00:05<00:11,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  42%|████▏     | 5/12 [00:07<00:10,  1.44s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  42%|████▏     | 5/12 [00:07<00:10,  1.47s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  42%|████▏     | 5/12 [00:07<00:10,  1.53s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  42%|████▏     | 5/12 [00:07<00:10,  1.53s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  42%|████▏     | 5/12 [00:07<00:10,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  42%|████▏     | 5/12 [00:07<00:10,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:  42%|████▏     | 5/12 [00:07<00:12,  1.71s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  50%|█████     | 6/12 [00:08<00:08,  1.42s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  50%|█████     | 6/12 [00:08<00:08,  1.47s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  50%|█████     | 6/12 [00:08<00:08,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  50%|█████     | 6/12 [00:08<00:09,  1.51s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  50%|█████     | 6/12 [00:08<00:09,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  50%|█████     | 6/12 [00:09<00:09,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:  50%|█████     | 6/12 [00:09<00:09,  1.64s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  58%|█████▊    | 7/12 [00:10<00:07,  1.45s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  58%|█████▊    | 7/12 [00:10<00:07,  1.49s/ba]\u001b[A\u001b[A\u001b[A\n",
      "#1:  58%|█████▊    | 7/12 [00:10<00:07,  1.47s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  58%|█████▊    | 7/12 [00:10<00:07,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  58%|█████▊    | 7/12 [00:10<00:07,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  58%|█████▊    | 7/12 [00:10<00:07,  1.53s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:  58%|█████▊    | 7/12 [00:11<00:08,  1.63s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  67%|██████▋   | 8/12 [00:11<00:05,  1.44s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  67%|██████▋   | 8/12 [00:11<00:05,  1.47s/ba]\u001b[A\u001b[A\u001b[A\n",
      "#1:  67%|██████▋   | 8/12 [00:11<00:05,  1.46s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  67%|██████▋   | 8/12 [00:11<00:06,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  67%|██████▋   | 8/12 [00:11<00:05,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  67%|██████▋   | 8/12 [00:12<00:06,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#0:  67%|██████▋   | 8/12 [00:12<00:06,  1.56s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  75%|███████▌  | 9/12 [00:13<00:04,  1.46s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  75%|███████▌  | 9/12 [00:13<00:04,  1.50s/ba]\u001b[A\u001b[A\u001b[A\n",
      "#1:  75%|███████▌  | 9/12 [00:13<00:04,  1.49s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  75%|███████▌  | 9/12 [00:13<00:04,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  75%|███████▌  | 9/12 [00:13<00:04,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  75%|███████▌  | 9/12 [00:13<00:04,  1.53s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#0:  75%|███████▌  | 9/12 [00:13<00:04,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  83%|████████▎ | 10/12 [00:14<00:02,  1.50s/ba]\u001b[A\u001b[A\n",
      "#1:  83%|████████▎ | 10/12 [00:14<00:02,  1.47s/ba]\u001b[A\n",
      "\n",
      "\n",
      "#3:  83%|████████▎ | 10/12 [00:14<00:03,  1.52s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  83%|████████▎ | 10/12 [00:14<00:03,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  83%|████████▎ | 10/12 [00:14<00:02,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  83%|████████▎ | 10/12 [00:15<00:03,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#0:  83%|████████▎ | 10/12 [00:15<00:03,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  92%|█████████▏| 11/12 [00:15<00:01,  1.46s/ba]\u001b[A\u001b[A\n",
      "#1:  92%|█████████▏| 11/12 [00:16<00:01,  1.49s/ba]\u001b[A\n",
      "\n",
      "\n",
      "#3:  92%|█████████▏| 11/12 [00:16<00:01,  1.53s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  92%|█████████▏| 11/12 [00:16<00:01,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  92%|█████████▏| 11/12 [00:16<00:01,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  92%|█████████▏| 11/12 [00:16<00:01,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#0:  92%|█████████▏| 11/12 [00:16<00:01,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2: 100%|██████████| 12/12 [00:17<00:00,  1.42s/ba]\u001b[A\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 12/12 [00:17<00:00,  1.43s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 12/12 [00:17<00:00,  1.45s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5: 100%|██████████| 12/12 [00:17<00:00,  1.45s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7: 100%|██████████| 12/12 [00:17<00:00,  1.46s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6: 100%|██████████| 12/12 [00:17<00:00,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#4: 100%|██████████| 12/12 [00:17<00:00,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#0: 100%|██████████| 12/12 [00:17<00:00,  1.50s/ba]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:   0%|          | 0/11 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "#0:   0%|          | 0/11 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\n",
      "#2:   0%|          | 0/11 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:   0%|          | 0/11 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:   0%|          | 0/11 [00:00<?, ?ba/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "#3:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[A\u001b[A\u001b[A\n",
      "#1:   0%|          | 0/11 [00:00<?, ?ba/s]\u001b[AAsking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#4:   9%|▉         | 1/11 [00:01<00:13,  1.38s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:   9%|▉         | 1/11 [00:01<00:13,  1.39s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:   9%|▉         | 1/11 [00:01<00:14,  1.44s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:   9%|▉         | 1/11 [00:01<00:14,  1.45s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:   9%|▉         | 1/11 [00:01<00:14,  1.45s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:   9%|▉         | 1/11 [00:01<00:14,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#0:  18%|█▊        | 2/11 [00:02<00:12,  1.36s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  18%|█▊        | 2/11 [00:02<00:12,  1.39s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  18%|█▊        | 2/11 [00:02<00:12,  1.39s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  18%|█▊        | 2/11 [00:02<00:12,  1.39s/ba]\u001b[A\u001b[A\n",
      "#1:  18%|█▊        | 2/11 [00:02<00:12,  1.42s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  18%|█▊        | 2/11 [00:02<00:12,  1.44s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  18%|█▊        | 2/11 [00:02<00:12,  1.44s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  18%|█▊        | 2/11 [00:02<00:13,  1.45s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  27%|██▋       | 3/11 [00:04<00:11,  1.40s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  27%|██▋       | 3/11 [00:04<00:11,  1.43s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  27%|██▋       | 3/11 [00:04<00:11,  1.43s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  27%|██▋       | 3/11 [00:04<00:11,  1.45s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  27%|██▋       | 3/11 [00:04<00:11,  1.44s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#0:  27%|██▋       | 3/11 [00:04<00:11,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  27%|██▋       | 3/11 [00:04<00:11,  1.47s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  36%|███▋      | 4/11 [00:05<00:10,  1.44s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#0:  36%|███▋      | 4/11 [00:05<00:10,  1.46s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  36%|███▋      | 4/11 [00:05<00:10,  1.47s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  36%|███▋      | 4/11 [00:05<00:10,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  36%|███▋      | 4/11 [00:05<00:10,  1.46s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  36%|███▋      | 4/11 [00:05<00:10,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#0:  45%|████▌     | 5/11 [00:07<00:08,  1.46s/ba]\u001b[A\n",
      "\n",
      "#2:  45%|████▌     | 5/11 [00:07<00:08,  1.48s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  45%|████▌     | 5/11 [00:07<00:08,  1.47s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  45%|████▌     | 5/11 [00:07<00:08,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  45%|████▌     | 5/11 [00:07<00:08,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  45%|████▌     | 5/11 [00:07<00:09,  1.51s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  45%|████▌     | 5/11 [00:07<00:09,  1.64s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  45%|████▌     | 5/11 [00:08<00:10,  1.78s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#0:  55%|█████▍    | 6/11 [00:08<00:07,  1.49s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  55%|█████▍    | 6/11 [00:08<00:07,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  55%|█████▍    | 6/11 [00:08<00:07,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  55%|█████▍    | 6/11 [00:08<00:07,  1.51s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  55%|█████▍    | 6/11 [00:08<00:07,  1.49s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  55%|█████▍    | 6/11 [00:09<00:07,  1.57s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#0:  64%|██████▎   | 7/11 [00:10<00:05,  1.45s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  64%|██████▎   | 7/11 [00:10<00:05,  1.48s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  64%|██████▎   | 7/11 [00:10<00:05,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  64%|██████▎   | 7/11 [00:10<00:05,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  64%|██████▎   | 7/11 [00:10<00:06,  1.52s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  64%|██████▎   | 7/11 [00:10<00:06,  1.55s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  64%|██████▎   | 7/11 [00:10<00:06,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  64%|██████▎   | 7/11 [00:11<00:06,  1.63s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#0:  73%|███████▎  | 8/11 [00:11<00:04,  1.50s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  73%|███████▎  | 8/11 [00:11<00:04,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  73%|███████▎  | 8/11 [00:11<00:04,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  73%|███████▎  | 8/11 [00:12<00:04,  1.55s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  73%|███████▎  | 8/11 [00:12<00:04,  1.55s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  73%|███████▎  | 8/11 [00:12<00:04,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#3:  73%|███████▎  | 8/11 [00:12<00:04,  1.57s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  82%|████████▏ | 9/11 [00:13<00:02,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#0:  82%|████████▏ | 9/11 [00:13<00:02,  1.50s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  82%|████████▏ | 9/11 [00:13<00:03,  1.53s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  82%|████████▏ | 9/11 [00:13<00:02,  1.49s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  82%|████████▏ | 9/11 [00:13<00:03,  1.56s/ba]\u001b[A\n",
      "\n",
      "\n",
      "#0:  91%|█████████ | 10/11 [00:14<00:01,  1.51s/ba][A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6:  91%|█████████ | 10/11 [00:14<00:01,  1.53s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#4:  91%|█████████ | 10/11 [00:14<00:01,  1.55s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#2:  91%|█████████ | 10/11 [00:15<00:01,  1.59s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5:  91%|█████████ | 10/11 [00:15<00:01,  1.52s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "#1:  91%|█████████ | 10/11 [00:15<00:01,  1.59s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  82%|████████▏ | 9/11 [00:15<00:04,  2.10s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#0: 100%|██████████| 11/11 [00:15<00:00,  1.45s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#6: 100%|██████████| 11/11 [00:16<00:00,  1.46s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#4: 100%|██████████| 11/11 [00:16<00:00,  1.47s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "#2: 100%|██████████| 11/11 [00:16<00:00,  1.47s/ba]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#5: 100%|██████████| 11/11 [00:16<00:00,  1.48s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "#1: 100%|██████████| 11/11 [00:16<00:00,  1.50s/ba]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "#3: 100%|██████████| 11/11 [00:16<00:00,  1.54s/ba]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7:  91%|█████████ | 10/11 [00:19<00:02,  2.79s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#7: 100%|██████████| 11/11 [00:22<00:00,  2.06s/ba]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading builder script: 100%|██████████| 4.20k/4.20k [00:00<00:00, 3.21MB/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 3.06 GiB (GPU 0; 31.75 GiB total capacity; 28.71 GiB already allocated; 2.21 GiB free; 28.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6d07f2f2b399>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;31m# Train the model, woohoo.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m trainer = RewardTrainer(\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplace_model_on_device\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_move_model_to_device\u001b[0;34m(self, model, device)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_move_model_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m         \u001b[0;31m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mParallelMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tie_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1747\u001b[0m             )\n\u001b[1;32m   1748\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhalf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    924\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.06 GiB (GPU 0; 31.75 GiB total capacity; 28.71 GiB already allocated; 2.21 GiB free; 28.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerBase,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers.utils import PaddingStrategy\n",
    "\n",
    "\n",
    "# Define and parse arguments.\n",
    "local_rank = 0\n",
    "resume_from_checkpoint = False\n",
    "deepspeed = None\n",
    "per_device_train_batch_size = 16\n",
    "per_device_eval_batch_size = 16\n",
    "gradient_accumulation_steps = 4\n",
    "learning_rate = 2e-5\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Simulated (cooking show) output from SFT (Step 1) for Amazon Customer Review summarization task\n",
    "model_name = \"bigscience/bloomz\" \n",
    "bf16 = False\n",
    "num_train_epochs = 5\n",
    "\n",
    "# Load the human comparisons dataset for tuning the reward model.\n",
    "# Simulated output from HF Step 2 to be used to train reward for \"helpfulness\"\n",
    "ds = load_dataset(\"openai/summarize_from_feedback\", name=\"comparisons\") \n",
    "\n",
    "# Define the training args. Needs to be done before the model is loaded if you are using deepspeed.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"{model_name.replace('/', '_')}_summarization_reward_model\",\n",
    "    learning_rate=learning_rate,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    weight_decay=weight_decay,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "#    deepspeed=deepspeed,\n",
    "#    local_rank=local_rank,\n",
    "    remove_unused_columns=False,\n",
    "    label_names=[],\n",
    ")\n",
    "\n",
    "# Load the value-head model and tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "# Need to do this for gpt2, because it doesn't have an official pad token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "# Turn the dataset into pairs of post + summaries, where text_j is the preferred post + summary and text_k is the other.\n",
    "def turn_into_text_classification_format(examples):\n",
    "    new_examples = {\"text_j\": [], \"text_k\": []}\n",
    "    for info, summaries, choice in zip(examples[\"info\"], examples[\"summaries\"], examples[\"choice\"]):\n",
    "        if len(summaries) != 2 or choice not in (0, 1):\n",
    "            raise ValueError(\n",
    "                f\"There should be two summaries with a choice that's either 0 or 1. Received {len(summaries)} summaries and choice={choice}.\"\n",
    "            )\n",
    "            \n",
    "        # TODO:  Change original_text_field = \"prompt\"\n",
    "        original_text_field = \"post\" if info[\"post\"] is not None else \"article\"\n",
    "\n",
    "        # TODO:  Change summaries[choice][\"text\"] to response\"prompt\"\n",
    "        new_examples[\"text_j\"].append(\n",
    "            summaries[choice][\"text\"] + \" \" + tokenizer.bos_token + \" \" + info[original_text_field]\n",
    "        )\n",
    "        new_examples[\"text_k\"].append(\n",
    "            summaries[0 if choice == 1 else 1][\"text\"] + \" \" + tokenizer.bos_token + \" \" + info[original_text_field]\n",
    "        )\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "\n",
    "num_proc = 8  # Can adjust to be higher if you have more processors. Should work even if you don't have 8 CPUs, though.\n",
    "original_columns = ds[\"train\"].column_names\n",
    "ds = ds.map(turn_into_text_classification_format, batched=True, num_proc=num_proc, remove_columns=original_columns)\n",
    "\n",
    "\n",
    "# Tokenize the dataset.\n",
    "def preprocess_function(examples):\n",
    "    tokenized_j = tokenizer(examples[\"text_j\"], truncation=True)\n",
    "    tokenized_k = tokenizer(examples[\"text_k\"], truncation=True)\n",
    "    return {\n",
    "        \"input_ids_j\": tokenized_j[\"input_ids\"],\n",
    "        \"attention_mask_j\": tokenized_j[\"attention_mask\"],\n",
    "        \"input_ids_k\": tokenized_k[\"input_ids\"],\n",
    "        \"attention_mask_k\": tokenized_k[\"attention_mask\"],\n",
    "    }\n",
    "\n",
    "\n",
    "tokenized_ds = ds.map(preprocess_function, batched=True, num_proc=num_proc, remove_columns=[\"text_j\", \"text_k\"])\n",
    "\n",
    "\n",
    "# We need to define a special data collator that batches the data in our j vs k format.\n",
    "@dataclass\n",
    "class RewardDataCollatorWithPadding:\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        features_j = []\n",
    "        features_k = []\n",
    "        for feature in features:\n",
    "            features_j.append({\"input_ids\": feature[\"input_ids_j\"], \"attention_mask\": feature[\"attention_mask_j\"]})\n",
    "            features_k.append({\"input_ids\": feature[\"input_ids_k\"], \"attention_mask\": feature[\"attention_mask_k\"]})\n",
    "        batch_j = self.tokenizer.pad(\n",
    "            features_j,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch_k = self.tokenizer.pad(\n",
    "            features_k,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=self.return_tensors,\n",
    "        )\n",
    "        batch = {\n",
    "            \"input_ids_j\": batch_j[\"input_ids\"],\n",
    "            \"attention_mask_j\": batch_j[\"attention_mask\"],\n",
    "            \"input_ids_k\": batch_k[\"input_ids\"],\n",
    "            \"attention_mask_k\": batch_k[\"attention_mask\"],\n",
    "            \"return_loss\": True,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "\n",
    "# Define the metric that we'll use for validation.\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, _ = eval_pred\n",
    "    # Here, predictions is rewards_j and rewards_k.\n",
    "    # We want to see how much of the time rewards_j > rewards_k.\n",
    "    predictions = np.argmax(predictions, axis=0)\n",
    "    labels = np.zeros(predictions.shape)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "class RewardTrainer(Trainer):\n",
    "    # Define how to compute the reward loss.\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        rewards_j = model(input_ids=inputs[\"input_ids_j\"], attention_mask=inputs[\"attention_mask_j\"])[0]\n",
    "        rewards_k = model(input_ids=inputs[\"input_ids_k\"], attention_mask=inputs[\"attention_mask_k\"])[0]\n",
    "        loss = -nn.functional.logsigmoid(rewards_j - rewards_k).mean()\n",
    "        if return_outputs:\n",
    "            return loss, {\"rewards_j\": rewards_j, \"rewards_k\": rewards_k}\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Train the model, woohoo.\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_ds[\"train\"],\n",
    "    eval_dataset=tokenized_ds[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=RewardDataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "trainer.train(resume_from_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b150c-c244-4e84-ab0a-155e55fe6a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-gpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
